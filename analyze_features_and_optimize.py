#!/usr/bin/env python3
"""
–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –∏–¥–µ–π.
–ê–Ω–∞–ª–∏–∑ –∑–∞ 3 –≥–æ–¥–∞ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏.
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import logging
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
from src.ml_models.advanced_xgboost_model import AdvancedEnsembleModel
from src.backtesting.aggressive_backtester import AggressiveBacktester
import warnings
warnings.filterwarnings('ignore')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(levelname)s | %(name)s | %(message)s')
logger = logging.getLogger(__name__)

class FeatureAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤."""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
    def create_test_data(self, days_back: int = 1095) -> dict:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∑–∞ 3 –≥–æ–¥–∞.
        
        Args:
            days_back: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –Ω–∞–∑–∞–¥ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1095 = 3 –≥–æ–¥–∞)
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –ø–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞–º
        """
        self.logger.info(f"üìä –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ {days_back} –¥–Ω–µ–π ({days_back/365:.1f} –ª–µ—Ç)")
        self.logger.info(f"‚è∞ –í—Ä–µ–º–µ–Ω–Ω–æ–π –ø—Ä–æ–º–µ–∂—É—Ç–æ–∫: {days_back} –¥–Ω–µ–π ({days_back * 24} —á–∞—Å–æ–≤)")
        
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days_back)
        
        self.logger.info(f"üìÖ –ü–µ—Ä–∏–æ–¥: {start_date.strftime('%Y-%m-%d %H:%M')} - {end_date.strftime('%Y-%m-%d %H:%M')}")
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è 5m, 15m, 1h
        data = {}
        
        for timeframe in ["5m", "15m", "1h"]:
            self.logger.info(f"üîÑ –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {timeframe}...")
            
            # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–≤–µ—á–µ–π
            if timeframe == "5m":
                periods = days_back * 24 * 12  # 12 —Å–≤–µ—á–µ–π –≤ —á–∞—Å
            elif timeframe == "15m":
                periods = days_back * 24 * 4   # 4 —Å–≤–µ—á–∏ –≤ —á–∞—Å
            else:  # 1h
                periods = days_back * 24       # 1 —Å–≤–µ—á–∞ –≤ —á–∞—Å
            
            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
            dates = pd.date_range(start=start_date, end=end_date, periods=periods)
            
            # –°–æ–∑–¥–∞–µ–º —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ —Ü–µ–Ω—ã BTC —Å —Ç—Ä–µ–Ω–¥–∞–º–∏ –∏ —Ü–∏–∫–ª–∞–º–∏
            np.random.seed(42)
            base_price = 45000
            
            # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã –∏ —Ü–∏–∫–ª—ã
            t = np.linspace(0, 1, len(dates))
            trend = 0.5 * np.sin(2 * np.pi * t * 2) + 0.3 * np.sin(2 * np.pi * t * 0.5)  # –î–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ —Ü–∏–∫–ª—ã
            returns = np.random.normal(0, 0.02, len(dates)) + 0.001 * trend  # –î–æ–±–∞–≤–ª—è–µ–º —Ç—Ä–µ–Ω–¥ –∫ –≤–æ–∑–≤—Ä–∞—Ç–∞–º
            
            prices = [base_price]
            for ret in returns[1:]:
                new_price = prices[-1] * (1 + ret)
                prices.append(max(new_price, 1000))  # –ú–∏–Ω–∏–º—É–º $1000
            
            # –°–æ–∑–¥–∞–µ–º OHLCV –¥–∞–Ω–Ω—ã–µ
            df_data = []
            for i, (date, price) in enumerate(zip(dates, prices)):
                # –°–æ–∑–¥–∞–µ–º —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ OHLCV
                volatility = abs(np.random.normal(0, 0.01))
                high = price * (1 + volatility)
                low = price * (1 - volatility)
                open_price = prices[i-1] if i > 0 else price
                volume = np.random.uniform(100, 1000)
                
                df_data.append({
                    'timestamp': date,
                    'open': open_price,
                    'high': high,
                    'low': low,
                    'close': price,
                    'volume': volume
                })
            
            df = pd.DataFrame(df_data)
            data[timeframe] = df
            self.logger.info(f"‚úÖ {timeframe}: {len(df)} —Å–≤–µ—á–µ–π")
        
        return data
    
    def create_advanced_features(self, data: dict) -> dict:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞."""
        self.logger.info("üîß –°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        
        features_data = {}
        for timeframe, df in data.items():
            self.logger.info(f"üìà –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {timeframe}...")
            
            # –°–æ–∑–¥–∞–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            df_features = df.copy()
            
            # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            df_features['price_change'] = df_features['close'].pct_change()
            df_features['volume_change'] = df_features['volume'].pct_change()
            df_features['high_low_ratio'] = df_features['high'] / df_features['low']
            df_features['close_open_ratio'] = df_features['close'] / df_features['open']
            
            # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
            df_features['volatility_5'] = df_features['price_change'].rolling(window=5).std()
            df_features['volatility_20'] = df_features['price_change'].rolling(window=20).std()
            df_features['volatility_50'] = df_features['price_change'].rolling(window=50).std()
            
            # –°–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ
            df_features['sma_10'] = df_features['close'].rolling(window=10).mean()
            df_features['sma_20'] = df_features['close'].rolling(window=20).mean()
            df_features['sma_50'] = df_features['close'].rolling(window=50).mean()
            df_features['sma_100'] = df_features['close'].rolling(window=100).mean()
            df_features['sma_200'] = df_features['close'].rolling(window=200).mean()
            
            # RSI
            delta = df_features['close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
            rs = gain / loss
            df_features['rsi'] = 100 - (100 / (1 + rs))
            
            # MACD
            exp1 = df_features['close'].ewm(span=12).mean()
            exp2 = df_features['close'].ewm(span=26).mean()
            df_features['macd'] = exp1 - exp2
            df_features['macd_signal'] = df_features['macd'].ewm(span=9).mean()
            df_features['macd_histogram'] = df_features['macd'] - df_features['macd_signal']
            
            # Bollinger Bands
            df_features['bb_middle'] = df_features['close'].rolling(window=20).mean()
            bb_std = df_features['close'].rolling(window=20).std()
            df_features['bb_upper'] = df_features['bb_middle'] + (bb_std * 2)
            df_features['bb_lower'] = df_features['bb_middle'] - (bb_std * 2)
            df_features['bb_position'] = (df_features['close'] - df_features['bb_lower']) / (df_features['bb_upper'] - df_features['bb_lower'])
            df_features['bb_width'] = (df_features['bb_upper'] - df_features['bb_lower']) / df_features['bb_middle']
            
            # Momentum
            df_features['momentum_5'] = df_features['close'] / df_features['close'].shift(5) - 1
            df_features['momentum_10'] = df_features['close'] / df_features['close'].shift(10) - 1
            df_features['momentum_20'] = df_features['close'] / df_features['close'].shift(20) - 1
            
            # Volume indicators
            df_features['volume_sma'] = df_features['volume'].rolling(window=20).mean()
            df_features['volume_ratio'] = df_features['volume'] / df_features['volume_sma']
            
            # –ù–û–í–´–ï –ü–†–ò–ó–ù–ê–ö–ò
            
            # 1. –°–µ–∑–æ–Ω–Ω–æ—Å—Ç—å
            df_features['hour'] = df_features['timestamp'].dt.hour
            df_features['day_of_week'] = df_features['timestamp'].dt.dayofweek
            df_features['month'] = df_features['timestamp'].dt.month
            
            # 2. –õ–∞–≥–∏ —Ü–µ–Ω
            for lag in [1, 2, 3, 5, 10, 20]:
                df_features[f'price_lag_{lag}'] = df_features['close'].shift(lag)
                df_features[f'return_lag_{lag}'] = df_features['price_change'].shift(lag)
            
            # 3. Rolling —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            for window in [5, 10, 20, 50]:
                df_features[f'rolling_min_{window}'] = df_features['close'].rolling(window=window).min()
                df_features[f'rolling_max_{window}'] = df_features['close'].rolling(window=window).max()
                df_features[f'rolling_mean_{window}'] = df_features['close'].rolling(window=window).mean()
                df_features[f'rolling_std_{window}'] = df_features['close'].rolling(window=window).std()
            
            # 4. –¢—Ä–µ–Ω–¥–æ–≤—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            # ADX (Average Directional Index)
            df_features['adx'] = self._calculate_adx(df_features, period=14)
            
            # Parabolic SAR
            df_features['sar'] = self._calculate_parabolic_sar(df_features)
            
            # 5. –û–±—ä—ë–º–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            # OBV (On-Balance Volume)
            df_features['obv'] = self._calculate_obv(df_features)
            
            # VWAP (Volume Weighted Average Price)
            df_features['vwap'] = self._calculate_vwap(df_features)
            
            # Money Flow Index
            df_features['mfi'] = self._calculate_mfi(df_features, period=14)
            
            # 6. –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
            # Realized Volatility
            df_features['realized_volatility'] = df_features['price_change'].rolling(window=20).apply(lambda x: np.sqrt(np.sum(x**2)))
            
            # 7. –ö—Ä–æ—Å—Å-–∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            df_features['price_vs_sma_ratio'] = df_features['close'] / df_features['sma_20']
            df_features['volume_price_trend'] = df_features['volume_ratio'] * df_features['price_change']
            
            # –£–¥–∞–ª—è–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è
            df_features = df_features.dropna()
            
            features_data[timeframe] = df_features
            self.logger.info(f"‚úÖ {timeframe}: {len(df_features)} —Å—Ç—Ä–æ–∫ —Å {len(df_features.columns)} –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏")
        
        return features_data
    
    def _calculate_adx(self, df: pd.DataFrame, period: int = 14) -> pd.Series:
        """–†–∞—Å—á–µ—Ç ADX (Average Directional Index)."""
        high = df['high']
        low = df['low']
        close = df['close']
        
        # True Range
        tr1 = high - low
        tr2 = abs(high - close.shift())
        tr3 = abs(low - close.shift())
        tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
        
        # Directional Movement
        dm_plus = high - high.shift()
        dm_minus = low.shift() - low
        dm_plus = dm_plus.where(dm_plus > dm_minus, 0)
        dm_minus = dm_minus.where(dm_minus > dm_plus, 0)
        
        # Smoothed values
        tr_smooth = tr.rolling(window=period).mean()
        dm_plus_smooth = dm_plus.rolling(window=period).mean()
        dm_minus_smooth = dm_minus.rolling(window=period).mean()
        
        # DI values
        di_plus = 100 * dm_plus_smooth / tr_smooth
        di_minus = 100 * dm_minus_smooth / tr_smooth
        
        # DX and ADX
        dx = 100 * abs(di_plus - di_minus) / (di_plus + di_minus)
        adx = dx.rolling(window=period).mean()
        
        return adx
    
    def _calculate_parabolic_sar(self, df: pd.DataFrame) -> pd.Series:
        """–†–∞—Å—á–µ—Ç Parabolic SAR."""
        high = df['high']
        low = df['low']
        close = df['close']
        
        # –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è Parabolic SAR
        sar = pd.Series(index=df.index, dtype=float)
        sar.iloc[0] = low.iloc[0]
        
        for i in range(1, len(df)):
            if close.iloc[i] > close.iloc[i-1]:  # –í–æ—Å—Ö–æ–¥—è—â–∏–π —Ç—Ä–µ–Ω–¥
                sar.iloc[i] = min(low.iloc[i], sar.iloc[i-1])
            else:  # –ù–∏—Å—Ö–æ–¥—è—â–∏–π —Ç—Ä–µ–Ω–¥
                sar.iloc[i] = max(high.iloc[i], sar.iloc[i-1])
        
        return sar
    
    def _calculate_obv(self, df: pd.DataFrame) -> pd.Series:
        """–†–∞—Å—á–µ—Ç OBV (On-Balance Volume)."""
        close = df['close']
        volume = df['volume']
        
        obv = pd.Series(index=df.index, dtype=float)
        obv.iloc[0] = volume.iloc[0]
        
        for i in range(1, len(df)):
            if close.iloc[i] > close.iloc[i-1]:
                obv.iloc[i] = obv.iloc[i-1] + volume.iloc[i]
            elif close.iloc[i] < close.iloc[i-1]:
                obv.iloc[i] = obv.iloc[i-1] - volume.iloc[i]
            else:
                obv.iloc[i] = obv.iloc[i-1]
        
        return obv
    
    def _calculate_vwap(self, df: pd.DataFrame) -> pd.Series:
        """–†–∞—Å—á–µ—Ç VWAP (Volume Weighted Average Price)."""
        typical_price = (df['high'] + df['low'] + df['close']) / 3
        vwap = (typical_price * df['volume']).rolling(window=20).sum() / df['volume'].rolling(window=20).sum()
        return vwap
    
    def _calculate_mfi(self, df: pd.DataFrame, period: int = 14) -> pd.Series:
        """–†–∞—Å—á–µ—Ç Money Flow Index."""
        typical_price = (df['high'] + df['low'] + df['close']) / 3
        money_flow = typical_price * df['volume']
        
        positive_flow = money_flow.where(typical_price > typical_price.shift(), 0).rolling(window=period).sum()
        negative_flow = money_flow.where(typical_price < typical_price.shift(), 0).rolling(window=period).sum()
        
        mfi = 100 - (100 / (1 + positive_flow / negative_flow))
        return mfi
    
    def create_target(self, df: pd.DataFrame, horizon: int = 12, threshold: float = 0.005) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π."""
        df_target = df.copy()
        
        # –°–æ–∑–¥–∞–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
        future_returns = df_target['close'].shift(-horizon) / df_target['close'] - 1
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        df_target['target'] = 0  # Hold
        df_target.loc[future_returns > threshold, 'target'] = 1  # Buy
        df_target.loc[future_returns < -threshold, 'target'] = 2  # Sell
        
        # –£–¥–∞–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏ –±–µ–∑ –±—É–¥—É—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        df_target = df_target.dropna()
        
        return df_target
    
    def filter_features_by_importance(self, importance_dict: dict, top_n: int = 20, threshold: float = 0.01) -> list:
        """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏."""
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
        sorted_features = sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –ø–æ—Ä–æ–≥—É
        filtered_features = [feature for feature, importance in sorted_features if importance > threshold]
        
        # –ë–µ—Ä–µ–º —Ç–æ–ø-N
        top_features = [feature for feature, _ in sorted_features[:top_n]]
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        final_features = list(set(filtered_features + top_features))
        
        return final_features
    
    def analyze_feature_importance(self, df: pd.DataFrame, method: str = 'xgboost') -> dict:
        """–ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤."""
        self.logger.info(f"üîç –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –º–µ—Ç–æ–¥–æ–º: {method}")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        feature_columns = [col for col in df.columns if col not in ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'target']]
        X = df[feature_columns].fillna(0)
        y = df['target']
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        X = X.replace([np.inf, -np.inf], 0)
        X = X.fillna(0)
        
        self.logger.info(f"üìä –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(feature_columns)}, –û–±—Ä–∞–∑—Ü–æ–≤: {len(X)}")
        self.logger.info(f"üéØ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {y.value_counts().to_dict()}")
        
        if method == 'xgboost':
            # XGBoost feature importance
            model = xgb.XGBClassifier(n_estimators=100, random_state=42)
            model.fit(X, y)
            importance = dict(zip(feature_columns, model.feature_importances_))
            
        elif method == 'random_forest':
            # Random Forest feature importance
            model = RandomForestClassifier(n_estimators=100, random_state=42)
            model.fit(X, y)
            importance = dict(zip(feature_columns, model.feature_importances_))
            
        elif method == 'mutual_info':
            # Mutual Information
            mi_scores = mutual_info_classif(X, y, random_state=42)
            importance = dict(zip(feature_columns, mi_scores))
            
        elif method == 'f_score':
            # F-score
            f_scores, _ = f_classif(X, y)
            importance = dict(zip(feature_columns, f_scores))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
        sorted_importance = dict(sorted(importance.items(), key=lambda x: x[1], reverse=True))
        
        return sorted_importance
    
    def plot_feature_importance(self, importance: dict, title: str = "Feature Importance"):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤."""
        plt.figure(figsize=(15, 10))
        
        # –¢–æ–ø-30 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        top_features = list(importance.keys())[:30]
        top_scores = list(importance.values())[:30]
        
        plt.barh(range(len(top_features)), top_scores)
        plt.yticks(range(len(top_features)), top_features)
        plt.xlabel('Importance Score')
        plt.title(title)
        plt.gca().invert_yaxis()
        
        plt.tight_layout()
        plt.savefig(f'feature_importance_{title.lower().replace(" ", "_")}.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        self.logger.info(f"üìä –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: feature_importance_{title.lower().replace(' ', '_')}.png")
    
    def optimize_parameters(self, df: pd.DataFrame) -> dict:
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏."""
        self.logger.info("‚öôÔ∏è –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏...")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        feature_columns = [col for col in df.columns if col not in ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'target']]
        X = df[feature_columns].fillna(0)
        y = df['target']
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        X = X.replace([np.inf, -np.inf], 0)
        X = X.fillna(0)
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
        split_idx = int(len(X) * 0.8)
        X_train, X_val = X[:split_idx], X[split_idx:]
        y_train, y_val = y[:split_idx], y[split_idx:]
        
        self.logger.info(f"üìä –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_train)}, –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è: {len(X_val)}")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        param_combinations = [
            {'n_estimators': 100, 'max_depth': 6, 'learning_rate': 0.1},
            {'n_estimators': 200, 'max_depth': 8, 'learning_rate': 0.05},
            {'n_estimators': 500, 'max_depth': 10, 'learning_rate': 0.03},
            {'n_estimators': 1000, 'max_depth': 12, 'learning_rate': 0.02},
            {'n_estimators': 2000, 'max_depth': 15, 'learning_rate': 0.015},
            {'n_estimators': 3000, 'max_depth': 20, 'learning_rate': 0.01},
        ]
        
        best_score = 0
        best_params = None
        results = []
        
        for params in param_combinations:
            self.logger.info(f"üß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {params}")
            
            model = xgb.XGBClassifier(**params, random_state=42)
            model.fit(X_train, y_train)
            
            score = model.score(X_val, y_val)
            results.append({'params': params, 'accuracy': score})
            
            if score > best_score:
                best_score = score
                best_params = params
        
        self.logger.info(f"üèÜ –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {best_params} (—Ç–æ—á–Ω–æ—Å—Ç—å: {best_score:.4f})")
        
        return {
            'best_params': best_params,
            'best_score': best_score,
            'all_results': results
        }
    
    def test_new_ideas(self, df: pd.DataFrame) -> dict:
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –∏–¥–µ–π."""
        self.logger.info("üí° –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –∏–¥–µ–π...")
        
        results = {}
        
        # –ò–¥–µ—è 1: –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        self.logger.info("üîß –ò–¥–µ—è 1: –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        df_combined = df.copy()
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        df_combined['rsi_momentum'] = df_combined['rsi'] * df_combined['momentum_5']
        df_combined['volume_price'] = df_combined['volume_ratio'] * df_combined['price_change']
        df_combined['bb_momentum'] = df_combined['bb_position'] * df_combined['momentum_10']
        df_combined['trend_strength'] = df_combined['adx'] * df_combined['price_vs_sma_ratio']
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å –Ω–æ–≤—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
        feature_columns = [col for col in df_combined.columns if col not in ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'target']]
        X = df_combined[feature_columns].fillna(0)
        y = df_combined['target']
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        X = X.replace([np.inf, -np.inf], 0)
        X = X.fillna(0)
        
        split_idx = int(len(X) * 0.8)
        X_train, X_val = X[:split_idx], X[split_idx:]
        y_train, y_val = y[:split_idx], y[split_idx:]
        
        model = xgb.XGBClassifier(n_estimators=500, max_depth=10, learning_rate=0.03, random_state=42)
        model.fit(X_train, y_train)
        score_combined = model.score(X_val, y_val)
        
        results['combined_features'] = score_combined
        self.logger.info(f"‚úÖ –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {score_combined:.4f}")
        
        # –ò–¥–µ—è 2: –í—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫–Ω–∞
        self.logger.info("‚è∞ –ò–¥–µ—è 2: –í—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫–Ω–∞")
        df_windows = df.copy()
        
        # –°–æ–∑–¥–∞–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ volatility
        df_windows['volatility_10'] = df_windows['price_change'].rolling(window=10).std()
        
        # –°–æ–∑–¥–∞–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ momentum
        df_windows['momentum_20'] = df_windows['close'] / df_windows['close'].shift(20) - 1
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫–Ω–∞—Ö
        for window in [5, 10, 20]:
            df_windows[f'volatility_{window}_norm'] = df_windows['volatility_20'] / df_windows[f'volatility_{window}']
            df_windows[f'momentum_{window}_norm'] = df_windows['momentum_10'] / df_windows[f'momentum_{window}']
        
        feature_columns = [col for col in df_windows.columns if col not in ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'target']]
        X = df_windows[feature_columns].fillna(0)
        y = df_windows['target']
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        X = X.replace([np.inf, -np.inf], 0)
        X = X.fillna(0)
        
        split_idx = int(len(X) * 0.8)
        X_train, X_val = X[:split_idx], X[split_idx:]
        y_train, y_val = y[:split_idx], y[split_idx:]
        
        model = xgb.XGBClassifier(n_estimators=500, max_depth=10, learning_rate=0.03, random_state=42)
        model.fit(X_train, y_train)
        score_windows = model.score(X_val, y_val)
        
        results['temporal_windows'] = score_windows
        self.logger.info(f"‚úÖ –í—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫–Ω–∞: {score_windows:.4f}")
        
        # –ò–¥–µ—è 3: –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –ø–æ—Ä–æ–≥–∏
        self.logger.info("üìä –ò–¥–µ—è 3: –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –ø–æ—Ä–æ–≥–∏")
        
        thresholds = [0.001, 0.002, 0.005, 0.01, 0.02, 0.03]
        threshold_scores = {}
        
        for threshold in thresholds:
            df_threshold = df.copy()
            future_returns = df_threshold['close'].shift(-12) / df_threshold['close'] - 1
            
            df_threshold['target'] = 0
            df_threshold.loc[future_returns > threshold, 'target'] = 1
            df_threshold.loc[future_returns < -threshold, 'target'] = 2
            df_threshold = df_threshold.dropna()
            
            if len(df_threshold) > 100:  # –ú–∏–Ω–∏–º—É–º –¥–∞–Ω–Ω—ã—Ö
                feature_columns = [col for col in df_threshold.columns if col not in ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'target']]
                X = df_threshold[feature_columns].fillna(0)
                y = df_threshold['target']
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                X = X.replace([np.inf, -np.inf], 0)
                X = X.fillna(0)
                
                split_idx = int(len(X) * 0.8)
                X_train, X_val = X[:split_idx], X[split_idx:]
                y_train, y_val = y[:split_idx], y[split_idx:]
                
                model = xgb.XGBClassifier(n_estimators=500, max_depth=10, learning_rate=0.03, random_state=42)
                model.fit(X_train, y_train)
                score = model.score(X_val, y_val)
                threshold_scores[threshold] = score
        
        best_threshold = max(threshold_scores, key=threshold_scores.get)
        results['adaptive_thresholds'] = {'best_threshold': best_threshold, 'scores': threshold_scores}
        self.logger.info(f"‚úÖ –õ—É—á—à–∏–π –ø–æ—Ä–æ–≥: {best_threshold} (—Ç–æ—á–Ω–æ—Å—Ç—å: {threshold_scores[best_threshold]:.4f})")
        
        return results
    
    def generate_strategy_recommendations(self, results: dict) -> dict:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ —É–ª—É—á—à–µ–Ω–∏—é —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏."""
        self.logger.info("üìã –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ —É–ª—É—á—à–µ–Ω–∏—é —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏...")
        
        recommendations = {
            'timeframe': results['timeframe'],
            'data_period': results['data_period'],
            'top_features': list(results['feature_importance_xgb'].keys())[:20],
            'best_params': results['optimization']['best_params'],
            'best_threshold': results['new_ideas']['adaptive_thresholds']['best_threshold'],
            'recommendations': []
        }
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        top_features = list(results['feature_importance_xgb'].keys())[:10]
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º
        recommendations['recommendations'].append({
            'category': '–ü—Ä–∏–∑–Ω–∞–∫–∏',
            'title': '–¢–æ–ø-10 –≤–∞–∂–Ω–µ–π—à–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤',
            'description': f'–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —ç—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–∏: {", ".join(top_features[:5])}...',
            'action': f'–û—Å—Ç–∞–≤—å—Ç–µ —Ç–æ–ª—å–∫–æ —Ç–æ–ø-20 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∏—Å–∫–ª—é—á–∏—Ç–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –≤–∞–∂–Ω–æ—Å—Ç—å—é < 0.01'
        })
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º
        best_params = results['optimization']['best_params']
        recommendations['recommendations'].append({
            'category': '–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏',
            'title': '–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã XGBoost',
            'description': f'–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: n_estimators={best_params["n_estimators"]}, max_depth={best_params["max_depth"]}, learning_rate={best_params["learning_rate"]}',
            'action': '–ü—Ä–∏–º–µ–Ω–∏—Ç–µ —ç—Ç–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏'
        })
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø–æ—Ä–æ–≥–∞–º
        best_threshold = results['new_ideas']['adaptive_thresholds']['best_threshold']
        recommendations['recommendations'].append({
            'category': '–ü–æ—Ä–æ–≥–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏',
            'title': '–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥',
            'description': f'–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø–æ—Ä–æ–≥ {best_threshold} –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å–∏–≥–Ω–∞–ª–æ–≤',
            'action': '–ù–∞—Å—Ç—Ä–æ–π—Ç–µ –ø–æ—Ä–æ–≥–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏ —Ä—ã–Ω–∫–∞'
        })
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
        if results['new_ideas']['combined_features'] > results['optimization']['best_score']:
            recommendations['recommendations'].append({
                'category': '–°—Ç—Ä–∞—Ç–µ–≥–∏—è',
                'title': '–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏',
                'description': '–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç',
                'action': '–î–æ–±–∞–≤—å—Ç–µ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å'
            })
        
        if results['new_ideas']['temporal_windows'] > results['optimization']['best_score']:
            recommendations['recommendations'].append({
                'category': '–°—Ç—Ä–∞—Ç–µ–≥–∏—è',
                'title': '–í—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫–Ω–∞',
                'description': '–í—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫–Ω–∞ —É–ª—É—á—à–∞—é—Ç —Ç–æ—á–Ω–æ—Å—Ç—å',
                'action': '–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫–Ω–∞—Ö'
            })
        
        # –û–±—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        recommendations['recommendations'].extend([
            {
                'category': '–û–±—â–∏–µ',
                'title': '–ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö',
                'description': '–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ SMOTE –∏–ª–∏ –¥—Ä—É–≥–∏–µ –º–µ—Ç–æ–¥—ã –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏',
                'action': '–ü—Ä–∏–º–µ–Ω–∏—Ç–µ SMOTE –¥–ª—è —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤'
            },
            {
                'category': '–û–±—â–∏–µ',
                'title': '–í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã',
                'description': '–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ TimeSeriesSplit –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏',
                'action': '–ó–∞–º–µ–Ω–∏—Ç–µ –æ–±—ã—á–Ω—É—é –≤–∞–ª–∏–¥–∞—Ü–∏—é –Ω–∞ –≤—Ä–µ–º–µ–Ω–Ω—É—é'
            },
            {
                'category': '–û–±—â–∏–µ',
                'title': '–ê–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ',
                'description': '–û–±—ä–µ–¥–∏–Ω–∏—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–µ–π',
                'action': '–°–æ–∑–¥–∞–π—Ç–µ –∞–Ω—Å–∞–º–±–ª—å –∏–∑ XGBoost, Random Forest –∏ LightGBM'
            }
        ])
        
        return recommendations
    
    def run_comprehensive_analysis(self, days_back: int = 1095):
        """–ó–∞–ø—É—Å–∫ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∑–∞ 3 –≥–æ–¥–∞."""
        self.logger.info("üöÄ –ó–ê–ü–£–°–ö –ö–û–ú–ü–õ–ï–ö–°–ù–û–ì–û –ê–ù–ê–õ–ò–ó–ê –ó–ê 3 –ì–û–î–ê")
        self.logger.info("=" * 80)
        
        # 1. –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        data = self.create_test_data(days_back)
        
        if not data:
            self.logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –¥–∞–Ω–Ω—ã–µ")
            return
        
        # 2. –°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        features_data = self.create_advanced_features(data)
        
        # 3. –ê–Ω–∞–ª–∏–∑ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞
        all_recommendations = {}
        
        for timeframe, df in features_data.items():
            self.logger.info(f"\nüìä –ê–ù–ê–õ–ò–ó –¢–ê–ô–ú–§–†–ï–ô–ú–ê: {timeframe}")
            self.logger.info("-" * 50)
            
            # –°–æ–∑–¥–∞–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
            df_target = self.create_target(df)
            
            # –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            importance_xgb = self.analyze_feature_importance(df_target, 'xgboost')
            importance_mi = self.analyze_feature_importance(df_target, 'mutual_info')
            
            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            top_features_xgb = self.filter_features_by_importance(importance_xgb, top_n=20, threshold=0.01)
            top_features_mi = self.filter_features_by_importance(importance_mi, top_n=20, threshold=0.01)
            
            self.logger.info(f"üîç –¢–æ–ø-20 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ XGBoost: {len(top_features_xgb)}")
            self.logger.info(f"üîç –¢–æ–ø-20 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ Mutual Info: {len(top_features_mi)}")
            
            # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
            self.plot_feature_importance(importance_xgb, f"XGBoost Feature Importance - {timeframe}")
            self.plot_feature_importance(importance_mi, f"Mutual Information - {timeframe}")
            
            # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            optimization = self.optimize_parameters(df_target)
            
            # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –∏–¥–µ–π
            new_ideas = self.test_new_ideas(df_target)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            results = {
                'timeframe': timeframe,
                'data_period': f"{days_back} days ({days_back/365:.1f} years)",
                'feature_importance_xgb': importance_xgb,
                'feature_importance_mi': importance_mi,
                'top_features_xgb': top_features_xgb,
                'top_features_mi': top_features_mi,
                'optimization': optimization,
                'new_ideas': new_ideas
            }
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
            recommendations = self.generate_strategy_recommendations(results)
            all_recommendations[timeframe] = recommendations
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª
            import json
            with open(f'analysis_results_{timeframe}_3years.json', 'w') as f:
                json.dump(results, f, indent=2, default=str)
            
            with open(f'recommendations_{timeframe}_3years.json', 'w') as f:
                json.dump(recommendations, f, indent=2, default=str)
            
            self.logger.info(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: analysis_results_{timeframe}_3years.json")
            self.logger.info(f"üíæ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: recommendations_{timeframe}_3years.json")
        
        # 4. –ò—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        self.logger.info("\nüìã –ò–¢–û–ì–û–í–´–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –£–õ–£–ß–®–ï–ù–ò–Æ –°–¢–†–ê–¢–ï–ì–ò–ò")
        self.logger.info("=" * 80)
        
        for timeframe, recs in all_recommendations.items():
            self.logger.info(f"\nüéØ –¢–ê–ô–ú–§–†–ï–ô–ú: {timeframe}")
            self.logger.info("-" * 30)
            
            for rec in recs['recommendations']:
                self.logger.info(f"üìå {rec['category']}: {rec['title']}")
                self.logger.info(f"   {rec['description']}")
                self.logger.info(f"   –î–µ–π—Å—Ç–≤–∏–µ: {rec['action']}")
                self.logger.info("")
        
        self.logger.info("\n‚úÖ –ö–û–ú–ü–õ–ï–ö–°–ù–´–ô –ê–ù–ê–õ–ò–ó –ó–ê 3 –ì–û–î–ê –ó–ê–í–ï–†–®–ï–ù")
        self.logger.info("=" * 80)

if __name__ == "__main__":
    analyzer = FeatureAnalyzer()
    analyzer.run_comprehensive_analysis(1095)  # 3 –≥–æ–¥–∞ 