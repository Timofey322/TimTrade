"""
ĞœĞ¾Ğ´ÑƒĞ»ÑŒ Ğ´Ğ»Ñ Ğ±Ğ°Ğ¹ĞµÑĞ¾Ğ²ÑĞºĞ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Optuna.
"""
import sys, os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))
import optuna
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score
import xgboost as xgb
import lightgbm as lgb
from sklearn.model_selection import cross_val_score

# --- Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ğ¿ÑƒÑ‚ÑŒ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¼ĞµĞ½ÑÑ‚ÑŒ) ---
df = pd.read_csv('data/historical/BTCUSDT_15m_5years_20210705_20250702.csv')
# Ğ—Ğ´ĞµÑÑŒ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ, Ñ‡Ñ‚Ğ¾ feature engineering ÑƒĞ¶Ğµ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ´Ñ‘Ğ½ Ğ¸ ĞµÑÑ‚ÑŒ X, y
# Ğ”Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°:
from core.timai_core import TimAIFeatureEngine
fe = TimAIFeatureEngine()
df_feat = fe.engineer_features(df)
returns = df_feat['close'].pct_change().shift(-1)
vol = returns.rolling(50).std()
target = np.where(returns > vol * 0.8, 2, np.where(returns > vol * 0.3, 1, np.where(returns < -vol * 0.8, 0, 1)))
df_feat = df_feat.iloc[:-1]
target = target[:-1]
valid_mask = ~(np.isnan(target) | np.isinf(target))
df_feat = df_feat[valid_mask]
target = target[valid_mask]
exclude_cols = ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'turnover', 'datetime']
X = df_feat.select_dtypes(include=[np.number])
X = X[[col for col in X.columns if col not in exclude_cols]]
y = target

X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=42)

def optimize_model(X, y, model_type='xgboost', n_trials=30, scoring='f1_weighted', random_state=42):
    """
    Ğ‘Ğ°Ğ¹ĞµÑĞ¾Ğ²ÑĞºĞ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ XGBoost Ğ¸Ğ»Ğ¸ LightGBM.
    Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹.
    """
    def objective(trial):
        if model_type == 'xgboost':
            params = {
                'n_estimators': trial.suggest_int('n_estimators', 50, 300),
                'max_depth': trial.suggest_int('max_depth', 3, 10),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
                'subsample': trial.suggest_float('subsample', 0.5, 1.0),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
                'reg_alpha': trial.suggest_float('reg_alpha', 0, 2),
                'reg_lambda': trial.suggest_float('reg_lambda', 0, 2),
                'random_state': random_state
            }
            model = xgb.XGBClassifier(**params, verbosity=0, n_jobs=-1)
        elif model_type == 'lightgbm':
            params = {
                'n_estimators': trial.suggest_int('n_estimators', 50, 300),
                'max_depth': trial.suggest_int('max_depth', 3, 10),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
                'subsample': trial.suggest_float('subsample', 0.5, 1.0),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
                'reg_alpha': trial.suggest_float('reg_alpha', 0, 2),
                'reg_lambda': trial.suggest_float('reg_lambda', 0, 2),
                'random_state': random_state
            }
            model = lgb.LGBMClassifier(**params)
        else:
            raise ValueError(f"Unknown model_type: {model_type}")
        score = cross_val_score(model, X, y, cv=3, scoring=scoring).mean()
        return score
    study = optuna.create_study(direction='maximize')
    study.optimize(objective, n_trials=n_trials)
    return study.best_params

# ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ:
# from optuna_hyperopt import optimize_model
# best_params = optimize_model(X, y, model_type='xgboost')

if __name__ == '__main__':
    print('ğŸ” Optuna hyperparameter search for XGBoost...')
    study_xgb = optuna.create_study(direction='maximize')
    study_xgb.optimize(objective_xgb, n_trials=30)
    print('Best XGBoost params:', study_xgb.best_params)

    print('ğŸ” Optuna hyperparameter search for LightGBM...')
    study_lgb = optuna.create_study(direction='maximize')
    study_lgb.optimize(objective_lgb, n_trials=30)
    print('Best LightGBM params:', study_lgb.best_params)

    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹
    import json
    with open('models/production/best_xgboost_params.json', 'w') as f:
        json.dump(study_xgb.best_params, f, indent=2)
    with open('models/production/best_lightgbm_params.json', 'w') as f:
        json.dump(study_lgb.best_params, f, indent=2) 