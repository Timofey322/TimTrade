#!/usr/bin/env python3
"""
ü§ñ TimAI - Advanced Multi-Model Trading System
–ú–Ω–æ–≥–æ–º–æ–¥–µ–ª—å–Ω–∞—è —Ç–æ—Ä–≥–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å XGBoost, LightGBM, Random Forest –∏ Incremental Learning
"""

import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from research.indicators.best_indicators import BEST_INDICATORS
from research.indicator_combinations.indicator_weights import INDICATOR_WEIGHTS
import numpy as np
import pandas as pd
import warnings
import time
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
import joblib
import json

warnings.filterwarnings('ignore')

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö ML –±–∏–±–ª–∏–æ—Ç–µ–∫
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
    print("‚úÖ XGBoost –¥–æ—Å—Ç—É–ø–µ–Ω:", xgb.__version__)
except ImportError:
    XGBOOST_AVAILABLE = False
    print("‚ùå XGBoost –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
    print("‚úÖ LightGBM –¥–æ—Å—Ç—É–ø–µ–Ω:", lgb.__version__)
except ImportError:
    LIGHTGBM_AVAILABLE = False
    print("‚ùå LightGBM –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

try:
    import optuna
    OPTUNA_AVAILABLE = True
    print("‚úÖ Optuna –¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
except ImportError:
    OPTUNA_AVAILABLE = False
    print("‚ùå Optuna –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

# --- –î–û–ë–ê–í–õ–ï–ù–û: –ò–º–ø–æ—Ä—Ç —É–ª—É—á—à–µ–Ω–∏–π ---
try:
    from research.advanced_techniques.smote_balancer import balance_data
    print("‚úÖ SMOTE/Over/Under Sampler –¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤")
except ImportError:
    balance_data = None
    print("‚ö†Ô∏è SMOTE/Over/Under Sampler –Ω–µ –Ω–∞–π–¥–µ–Ω")
try:
    from research.advanced_techniques.optuna_hyperopt import optimize_model
    print("‚úÖ Optuna Hyperopt –¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
except ImportError:
    optimize_model = None
    print("‚ö†Ô∏è Optuna Hyperopt –Ω–µ –Ω–∞–π–¥–µ–Ω")

# –ë–∞–∑–æ–≤—ã–µ ML –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
from sklearn.linear_model import SGDClassifier, PassiveAggressiveClassifier
from sklearn.metrics import accuracy_score, f1_score, classification_report, precision_recall_fscore_support
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler

class TimAIFeatureEngine:
    """üîß TimAI Feature Engineering - –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –∏–Ω–∂–µ–Ω–µ—Ä–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
    
    def __init__(self):
        self.feature_names = []
        self.scaler = StandardScaler()
        
    def engineer_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–µ—Ç –ø–æ–ª–Ω—ã–π –Ω–∞–±–æ—Ä —Ç–æ—Ä–≥–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        print("üîß TimAI Feature Engineering - —Å–æ–∑–¥–∞–Ω–∏–µ 80+ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        
        # –î–æ–±–∞–≤–ª—è–µ–º datetime –µ—Å–ª–∏ –µ—Å—Ç—å timestamp
        if 'datetime' in df.columns:
            df['datetime'] = pd.to_datetime(df['datetime'])  # –£–∂–µ —Å—Ç—Ä–æ–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ datetime
        elif 'timestamp' in df.columns:
            df['datetime'] = pd.to_datetime(df['timestamp'], unit='ms')  # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: unit='ms'
        
        # 1. –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        df = self._technical_indicators(df)
        
        # 2. –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å –∏ —Ä–∏—Å–∫
        df = self._volatility_features(df)
        
        # 3. Volume –∞–Ω–∞–ª–∏–∑
        df = self._volume_features(df)
        
        # 4. –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        df = self._temporal_features(df)
        
        # 5. –†—ã–Ω–æ—á–Ω—ã–µ —Ä–µ–∂–∏–º—ã
        df = self._market_regimes(df)
        
        # 6. –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        df = self._feature_interactions(df)
        
        # 7. –ú–∏–∫—Ä–æ—Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        df = self._microstructure_features(df)
        
        # 8. –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        df = self._advanced_indicators(df)
        
        # 7. 1D CNN –¥–ª—è –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ (–µ—Å–ª–∏ TensorFlow –¥–æ—Å—Ç—É–ø–µ–Ω)
        try:
            import tensorflow as tf
            from tensorflow.keras.models import Sequential
            from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense, Dropout
            
            # –ü—Ä–æ—Å—Ç–∞—è 1D CNN –¥–ª—è price patterns
            def create_simple_cnn():
                model = Sequential([
                    Conv1D(64, 3, activation='relu', input_shape=(20, 5)),  # 20 —Å–≤–µ—á–µ–π, 5 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (OHLCV)
                    MaxPooling1D(2),
                    Conv1D(128, 3, activation='relu'),
                    GlobalMaxPooling1D(),
                    Dense(64, activation='relu'),
                    Dropout(0.3),
                    Dense(3, activation='softmax')  # 3 –∫–ª–∞—Å—Å–∞: 0=Sell, 1=Hold, 2=Buy
                ])
                model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')
                return model
            
            # –°–æ–∑–¥–∞–µ–º CNN –º–æ–¥–µ–ª—å (–±—É–¥–µ—Ç –æ–±—É—á–∞—Ç—å—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ)
            self.cnn_model = create_simple_cnn()
            self.has_cnn = True
            print("   ‚úÖ 1D CNN –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è price patterns")
            
        except ImportError:
            self.has_cnn = False
            print("   ‚ö†Ô∏è TensorFlow –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, CNN –ø—Ä–æ–ø—É—â–µ–Ω")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–∫–∞–ª—å–ø–∏–Ω–≥–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        df = self._add_scalping_features(df)
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º –º–æ–¥–µ–ª–µ–π
        df = df.ffill().bfill().fillna(0)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –¥–ª—è –≥–∞—Ä–∞–Ω—Ç–∏–∏ —Ä–∞–±–æ—Ç—ã –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
        df = self._clean_features(df)
        
        print(f"   ‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(df.columns) - 6} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è TimAI")
        # --- –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ª—É—á—à–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ + OHLCV + datetime ---
        base_cols = ['open', 'high', 'low', 'close', 'volume', 'datetime']
        keep_cols = base_cols + [col for col in df.columns if col in BEST_INDICATORS]
        df = df[[col for col in keep_cols if col in df.columns]]
        print(f"   ‚úÖ –û—Ç–æ–±—Ä–∞–Ω–æ {len(df.columns) - 6} –ª—É—á—à–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {BEST_INDICATORS}")
        
        # --- –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤–µ—Å–∞–º–∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ ---
        for col in df.columns:
            if col in INDICATOR_WEIGHTS and col not in base_cols:
                weight = INDICATOR_WEIGHTS[col]
                df[f'{col}_weighted'] = df[col] * weight
                print(f"   ‚öñÔ∏è –î–æ–±–∞–≤–ª–µ–Ω –≤–∑–≤–µ—à–µ–Ω–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫: {col}_weighted (–≤–µ—Å: {weight:.3f})")
        
        return df
    
    def _technical_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ë–∞–∑–æ–≤—ã–µ –∏ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã"""
        
        # EMA —Å–∏—Å—Ç–µ–º–∞
        ema_periods = [5, 8, 13, 21, 34, 55, 89]
        for period in ema_periods:
            df[f'ema_{period}'] = df['close'].ewm(span=period, min_periods=1).mean()
            df[f'ema_ratio_{period}'] = df['close'] / (df[f'ema_{period}'] + 1e-8)
        
        # SMA —Å–∏—Å—Ç–µ–º–∞
        sma_periods = [10, 20, 50, 100, 200]
        for period in sma_periods:
            df[f'sma_{period}'] = df['close'].rolling(period, min_periods=1).mean()
            df[f'sma_ratio_{period}'] = df['close'] / (df[f'sma_{period}'] + 1e-8)
        
        # EMA –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è
        df['ema_cross_fast'] = (df['ema_8'] > df['ema_21']).astype(int)
        df['ema_cross_medium'] = (df['ema_21'] > df['ema_55']).astype(int)
        df['ema_cross_slow'] = (df['ema_55'] > df['sma_200']).astype(int)
        
        # RSI —Å–µ–º–µ–π—Å—Ç–≤–æ
        rsi_periods = [7, 14, 21, 28]
        for period in rsi_periods:
            delta = df['close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(period, min_periods=1).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(period, min_periods=1).mean()
            rs = gain / (loss + 1e-8)
            df[f'rsi_{period}'] = 100 - (100 / (1 + rs))
        
        # RSI –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏—è –∏ –º–æ–º–µ–Ω—Ç—É–º
        df['rsi_divergence'] = df['rsi_14'] - df['rsi_14'].rolling(10, min_periods=1).mean()
        df['rsi_momentum'] = df['rsi_14'].diff()
        df['rsi_volatility'] = df['rsi_14'].rolling(20, min_periods=1).std()
        
        # MACD —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π
        ema12 = df['close'].ewm(span=12, min_periods=1).mean()
        ema26 = df['close'].ewm(span=26, min_periods=1).mean()
        df['macd'] = ema12 - ema26
        df['macd_signal'] = df['macd'].ewm(span=9, min_periods=1).mean()
        df['macd_histogram'] = df['macd'] - df['macd_signal']
        df['macd_momentum'] = df['macd_histogram'].diff()
        df['macd_acceleration'] = df['macd_momentum'].diff()
        
        # Bollinger Bands —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ
        bb_periods = [20, 50]
        for period in bb_periods:
            sma = df['close'].rolling(period, min_periods=1).mean()
            std = df['close'].rolling(period, min_periods=1).std()
            df[f'bb_upper_{period}'] = sma + 2 * std
            df[f'bb_lower_{period}'] = sma - 2 * std
            df[f'bb_position_{period}'] = (df['close'] - df[f'bb_lower_{period}']) / (df[f'bb_upper_{period}'] - df[f'bb_lower_{period}'] + 1e-8)
            df[f'bb_width_{period}'] = (df[f'bb_upper_{period}'] - df[f'bb_lower_{period}']) / (sma + 1e-8)
            df[f'bb_squeeze_{period}'] = (df[f'bb_width_{period}'] < df[f'bb_width_{period}'].rolling(20, min_periods=1).mean()).astype(int)
        
        # Stochastic —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π
        periods = [14, 21]
        for period in periods:
            low_min = df['low'].rolling(period, min_periods=1).min()
            high_max = df['high'].rolling(period, min_periods=1).max()
            df[f'stoch_k_{period}'] = 100 * (df['close'] - low_min) / (high_max - low_min + 1e-8)
            df[f'stoch_d_{period}'] = df[f'stoch_k_{period}'].rolling(3, min_periods=1).mean()
        
        return df
    
    def _volatility_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–Ω–∞–ª–∏–∑ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏"""
        
        returns = df['close'].pct_change().fillna(0)
        
        # –†–∞–∑–ª–∏—á–Ω—ã–µ –ø–µ—Ä–∏–æ–¥—ã –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
        vol_periods = [5, 10, 20, 50, 100]
        for period in vol_periods:
            df[f'volatility_{period}'] = returns.rolling(period, min_periods=1).std()
            df[f'vol_percentile_{period}'] = df[f'volatility_{period}'].rolling(100, min_periods=1).rank(pct=True)
        
        # Volatility regimes
        df['vol_regime_short'] = df['volatility_20'] / (df['volatility_100'] + 1e-8)
        df['vol_regime_trend'] = df['volatility_20'].rolling(10, min_periods=1).mean() / (df['volatility_20'] + 1e-8)
        df['vol_regime_acceleration'] = df['vol_regime_short'].diff()
        
        # Parkinson volatility (high-low)
        df['parkinson_vol'] = np.sqrt(0.361 * np.log((df['high'] / df['low']).clip(lower=1e-8)) ** 2)
        
        # Garman-Klass volatility
        df['gk_vol'] = np.sqrt(
            0.5 * np.log((df['high'] / df['low']).clip(lower=1e-8)) ** 2 -
            (2 * np.log(2) - 1) * np.log((df['close'] / df['open']).clip(lower=1e-8)) ** 2
        )
        
        # Realized volatility
        df['realized_vol'] = returns.rolling(20, min_periods=1).apply(lambda x: np.sqrt(np.sum(x**2)))
        
        return df
    
    def _volume_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π volume –∞–Ω–∞–ª–∏–∑"""
        
        # Volume moving averages
        vol_periods = [5, 10, 20, 50, 100]
        for period in vol_periods:
            df[f'volume_ma_{period}'] = df['volume'].rolling(period, min_periods=1).mean()
            df[f'volume_ratio_{period}'] = df['volume'] / (df[f'volume_ma_{period}'] + 1e-8)
        
        # Volume trends –∏ momentum
        df['volume_trend'] = df['volume'].rolling(10, min_periods=1).apply(lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) > 1 else 0)
        df['volume_momentum'] = df['volume'].pct_change().fillna(0)
        df['volume_acceleration'] = df['volume_momentum'].diff()
        
        # Price-Volume relationships
        df['price_volume_trend'] = df['close'].pct_change().fillna(0) * df['volume_ratio_20']
        df['volume_price_correlation'] = df['close'].pct_change().fillna(0).rolling(20, min_periods=1).corr(df['volume'].pct_change().fillna(0))
        
        # Volume regime analysis
        volume_ma_100 = df['volume'].rolling(100, min_periods=1).mean()
        df['volume_regime'] = df['volume_ma_20'] / (volume_ma_100 + 1e-8)
        df['volume_outlier'] = (df['volume'] > df['volume_ma_20'] + 2 * df['volume'].rolling(20, min_periods=1).std()).astype(int)
        
        # Money flow –∏ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ
        typical_price = (df['high'] + df['low'] + df['close']) / 3
        money_flow = typical_price * df['volume']
        df['money_flow_ratio'] = money_flow / (money_flow.rolling(20, min_periods=1).mean() + 1e-8)
        
        # On Balance Volume
        df['obv'] = (df['volume'] * np.where(df['close'] > df['close'].shift(1), 1, 
                                           np.where(df['close'] < df['close'].shift(1), -1, 0))).cumsum()
        df['obv_ma'] = df['obv'].rolling(20, min_periods=1).mean()
        df['obv_divergence'] = df['obv'] - df['obv_ma']
        
        return df
    
    def _temporal_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∏ —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å"""
        
        if 'datetime' in df.columns:
            df['hour'] = df['datetime'].dt.hour
            df['day_of_week'] = df['datetime'].dt.dayofweek
            df['day_of_month'] = df['datetime'].dt.day
            df['month'] = df['datetime'].dt.month
            df['quarter'] = df['datetime'].dt.quarter
            
            # –¶–∏–∫–ª–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)
            df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)
            df['dow_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)
            df['dow_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)
            df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)
            df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)
            
            # –¢–æ—Ä–≥–æ–≤—ã–µ —Å–µ—Å—Å–∏–∏
            df['asian_session'] = ((df['hour'] >= 0) & (df['hour'] < 8)).astype(float)
            df['european_session'] = ((df['hour'] >= 8) & (df['hour'] < 16)).astype(float)
            df['american_session'] = ((df['hour'] >= 16) & (df['hour'] < 24)).astype(float)
            
            # Weekend –∏ –ø—Ä–∞–∑–¥–Ω–∏—á–Ω—ã–µ —ç—Ñ—Ñ–µ–∫—Ç—ã
            df['is_weekend'] = (df['day_of_week'] >= 5).astype(float)
            df['is_month_end'] = (df['day_of_month'] >= 28).astype(float)
            df['is_quarter_end'] = (df['month'] % 3 == 0).astype(float)
        
        return df
    
    def _market_regimes(self, df: pd.DataFrame) -> pd.DataFrame:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä—ã–Ω–æ—á–Ω—ã—Ö —Ä–µ–∂–∏–º–æ–≤"""
        
        returns = df['close'].pct_change().fillna(0)
        
        # Trend detection
        trend_periods = [10, 20, 50, 100]
        for period in trend_periods:
            df[f'trend_{period}'] = df['close'].rolling(period, min_periods=1).apply(
                lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) > 1 else 0
            )
        
        # Trend strength –∏ direction
        df['trend_strength'] = abs(df['trend_20']) / (df['volatility_20'] + 1e-8)
        df['trend_consistency'] = (df['trend_10'] * df['trend_20'] > 0).astype(int)
        
        # Market state classification
        df['market_state'] = np.where(df['trend_20'] > df['volatility_20'], 2,      # Strong Bull
                                    np.where(df['trend_20'] > df['volatility_20'] * 0.5, 1,  # Bull
                                           np.where(df['trend_20'] < -df['volatility_20'], -2, # Strong Bear
                                                  np.where(df['trend_20'] < -df['volatility_20'] * 0.5, -1, 0)))) # Bear, Sideways
        
        # Volatility regime
        vol_mean = df['volatility_20'].rolling(100, min_periods=1).mean()
        df['vol_regime'] = np.where(df['volatility_20'] > vol_mean * 1.5, 2,  # High vol
                                  np.where(df['volatility_20'] < vol_mean * 0.5, 0, 1))  # Low vol, Normal
        
        # Mean reversion vs momentum
        df['mean_reversion_signal'] = (df['close'] - df['sma_20']) / (df['volatility_20'] + 1e-8)
        df['momentum_signal'] = df['close'].pct_change(10).fillna(0)
        
        return df
    
    def _feature_interactions(self, df: pd.DataFrame) -> pd.DataFrame:
        """–í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –º–µ–∂–¥—É –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏"""
        
        # RSI-Volume interactions
        df['rsi_volume_interaction'] = df['rsi_14'] * df['volume_ratio_20']
        df['rsi_volume_divergence'] = df['rsi_14'] - df['volume_ratio_20'] * 50
        
        # Volatility-Trend interactions
        df['vol_trend_interaction'] = df['vol_regime_short'] * df['trend_strength']
        df['vol_trend_divergence'] = df['volatility_20'] - abs(df['trend_20'])
        
        # Price momentum combinations
        df['price_momentum'] = df['close'].pct_change() * df['volume_ratio_20']
        df['price_momentum_acceleration'] = df['price_momentum'].diff()
        
        # MACD-Volume confluence
        df['macd_volume_confluence'] = df['macd_histogram'] * df['volume_regime']
        df['macd_volume_divergence'] = np.sign(df['macd_histogram']) != np.sign(df['volume_trend'])
        
        # BB-RSI interactions
        df['bb_rsi_interaction'] = df['bb_position_20'] * (df['rsi_14'] - 50) / 50
        df['bb_rsi_squeeze'] = (df['bb_squeeze_20'] & (df['rsi_14'] > 30) & (df['rsi_14'] < 70)).astype(int)
        
        # Trend-Volume confirmation
        df['trend_volume_confirm'] = df['trend_20'] * df['volume_regime']
        df['trend_volume_divergence'] = (np.sign(df['trend_20']) != np.sign(df['volume_trend'])).astype(int)
        
        return df
    
    def _microstructure_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ú–∏–∫—Ä–æ—Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Ä—ã–Ω–∫–∞"""
        
        # Bid-Ask spread proxy
        df['hl_spread'] = (df['high'] - df['low']) / df['close']
        df['hl_spread_ma'] = df['hl_spread'].rolling(20, min_periods=1).mean()
        df['hl_spread_normalized'] = df['hl_spread'] / (df['hl_spread_ma'] + 1e-8)
        
        # Price impact
        df['price_impact'] = abs(df['close'].pct_change()) / (df['volume_ratio_20'] + 1e-8)
        df['price_impact_ma'] = df['price_impact'].rolling(20, min_periods=1).mean()
        
        # Order flow imbalance proxy
        df['order_flow_imbalance'] = (df['close'] - df['open']) / (df['high'] - df['low'] + 1e-8)
        df['order_flow_persistence'] = df['order_flow_imbalance'].rolling(5, min_periods=1).mean()
        
        # Tick momentum
        tick_periods = [5, 10, 20]
        for period in tick_periods:
            df[f'tick_momentum_{period}'] = (df['close'] > df['close'].shift(1)).rolling(period, min_periods=1).sum() - period/2
        
        # Intrabar pressure
        df['buying_pressure'] = (df['close'] - df['low']) / (df['high'] - df['low'] + 1e-8)
        df['selling_pressure'] = (df['high'] - df['close']) / (df['high'] - df['low'] + 1e-8)
        df['pressure_ratio'] = df['buying_pressure'] / (df['selling_pressure'] + 1e-8)
        
        return df
    
    def _advanced_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã"""
        
        # Commodity Channel Index
        typical_price = (df['high'] + df['low'] + df['close']) / 3
        sma_tp = typical_price.rolling(20, min_periods=1).mean()
        mad = typical_price.rolling(20, min_periods=1).apply(lambda x: np.mean(np.abs(x - x.mean())))
        df['cci'] = (typical_price - sma_tp) / (0.015 * mad)
        
        # Williams %R
        periods = [14, 21]
        for period in periods:
            high_max = df['high'].rolling(period, min_periods=1).max()
            low_min = df['low'].rolling(period, min_periods=1).min()
            df[f'williams_r_{period}'] = -100 * (high_max - df['close']) / (high_max - low_min + 1e-8)
        
        # Average True Range
        tr1 = df['high'] - df['low']
        tr2 = abs(df['high'] - df['close'].shift(1))
        tr3 = abs(df['low'] - df['close'].shift(1))
        true_range = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
        df['atr'] = true_range.rolling(14, min_periods=1).mean()
        df['atr_ratio'] = df['atr'] / df['close']
        
        # Momentum oscillators
        df['momentum_10'] = df['close'] / df['close'].shift(10) - 1
        df['roc_10'] = df['close'].pct_change(10)
        df['roc_20'] = df['close'].pct_change(20)
        
        return df
    
    def _clean_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–û—á–∏—Å—Ç–∫–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        
        # –ó–∞–º–µ–Ω—è–µ–º inf –Ω–∞ NaN
        df = df.replace([np.inf, -np.inf], np.nan)
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN
        df = df.ffill().bfill().fillna(0)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≤—ã–±—Ä–æ—Å—ã –±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        exclude_cols = ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'turnover']
        
        for col in numeric_cols:
            if col not in exclude_cols:
                # –ë–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤
                q99 = df[col].quantile(0.99)   # –ë—ã–ª–æ 0.995
                q01 = df[col].quantile(0.01)   # –ë—ã–ª–æ 0.005
                df[col] = df[col].clip(q01, q99)
                
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ infinity –ø–æ—Å–ª–µ clip
                df[col] = df[col].replace([np.inf, -np.inf], np.nan).fillna(0)
                
                # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –µ—Å–ª–∏ –≤—Å–µ –µ—â–µ –µ—Å—Ç—å –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                if not np.isfinite(df[col]).all():
                    print(f"   ‚ö†Ô∏è –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –∫–æ–ª–æ–Ω–∫–∏: {col}")
                    df[col] = np.where(np.isfinite(df[col]), df[col], 0)
        
        return df

    def _scalping_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–æ–π —Ç–æ—Ä–≥–æ–≤–ª–∏"""
        
        # –ö—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—ã–π momentum –∏ acceleration
        df['price_velocity_1'] = df['close'].pct_change(1)  # –ó–∞ 1 —Å–≤–µ—á—É
        df['price_velocity_3'] = df['close'].pct_change(3)  # –ó–∞ 3 —Å–≤–µ—á–∏
        df['price_velocity_5'] = df['close'].pct_change(5)  # –ó–∞ 5 —Å–≤–µ—á–µ–π
        
        df['price_acceleration_1'] = df['price_velocity_1'].diff()
        df['price_acceleration_3'] = df['price_velocity_3'].diff()
        
        # Volume surges –¥–ª—è —Å–∫–∞–ª—å–ø–∏–Ω–≥–∞
        df['volume_surge_2'] = df['volume'] / (df['volume'].rolling(2, min_periods=1).mean() + 1e-8)
        df['volume_surge_5'] = df['volume'] / (df['volume'].rolling(5, min_periods=1).mean() + 1e-8)
        df['volume_surge_10'] = df['volume'] / (df['volume'].rolling(10, min_periods=1).mean() + 1e-8)
        
        # Recent high/low touches (–∫–∞—Å–∞–Ω–∏—è —É—Ä–æ–≤–Ω–µ–π)
        recent_high_5 = df['high'].rolling(5, min_periods=1).max()
        recent_low_5 = df['low'].rolling(5, min_periods=1).min()
        recent_high_10 = df['high'].rolling(10, min_periods=1).max()
        recent_low_10 = df['low'].rolling(10, min_periods=1).min()
        
        df['near_recent_high_5'] = (abs(df['close'] - recent_high_5) / df['close'] < 0.005).astype(int)
        df['near_recent_low_5'] = (abs(df['close'] - recent_low_5) / df['close'] < 0.005).astype(int)
        df['near_recent_high_10'] = (abs(df['close'] - recent_high_10) / df['close'] < 0.005).astype(int)
        df['near_recent_low_10'] = (abs(df['close'] - recent_low_10) / df['close'] < 0.005).astype(int)
        
        # Breakout detection –¥–ª—è —Å–∫–∞–ª—å–ø–∏–Ω–≥–∞
        df['breakout_up_5'] = (df['close'] > recent_high_5.shift(1)).astype(int)
        df['breakout_down_5'] = (df['close'] < recent_low_5.shift(1)).astype(int)
        df['breakout_up_10'] = (df['close'] > recent_high_10.shift(1)).astype(int)
        df['breakout_down_10'] = (df['close'] < recent_low_10.shift(1)).astype(int)
        
        # Volume-confirmed moves
        df['volume_confirmed_up'] = (
            (df['close'] > df['open']) & 
            (df['volume'] > df['volume'].rolling(5, min_periods=1).mean()) &
            (df['price_velocity_1'] > 0)
        ).astype(int)
        
        df['volume_confirmed_down'] = (
            (df['close'] < df['open']) & 
            (df['volume'] > df['volume'].rolling(5, min_periods=1).mean()) &
            (df['price_velocity_1'] < 0)
        ).astype(int)
        
        # Scalping momentum indicators
        df['scalp_momentum_short'] = (
            df['price_velocity_1'] * df['volume_surge_2']
        )
        
        df['scalp_momentum_medium'] = (
            df['price_velocity_3'] * df['volume_surge_5']
        )
        
        # Intrabar strength –¥–ª—è —Å–∫–∞–ª—å–ø–∏–Ω–≥–∞
        df['intrabar_strength'] = (df['close'] - df['open']) / (df['high'] - df['low'] + 1e-8)
        df['intrabar_volume_ratio'] = df['volume'] / (df['turnover'] / df['close'] + 1e-8)
        
        # Quick reversal signals
        df['quick_reversal_up'] = (
            (df['close'].shift(1) < df['open'].shift(1)) &  # –ü—Ä–µ–¥—ã–¥—É—â–∞—è –∫—Ä–∞—Å–Ω–∞—è
            (df['close'] > df['open']) &                     # –¢–µ–∫—É—â–∞—è –∑–µ–ª–µ–Ω–∞—è
            (df['volume'] > df['volume'].shift(1)) &         # –û–±—ä–µ–º —Ä–∞—Å—Ç–µ—Ç
            (df['close'] > df['high'].shift(1))              # –í—ã—à–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –º–∞–∫—Å–∏–º—É–º–∞
        ).astype(int)
        
        df['quick_reversal_down'] = (
            (df['close'].shift(1) > df['open'].shift(1)) &  # –ü—Ä–µ–¥—ã–¥—É—â–∞—è –∑–µ–ª–µ–Ω–∞—è
            (df['close'] < df['open']) &                     # –¢–µ–∫—É—â–∞—è –∫—Ä–∞—Å–Ω–∞—è
            (df['volume'] > df['volume'].shift(1)) &         # –û–±—ä–µ–º —Ä–∞—Å—Ç–µ—Ç
            (df['close'] < df['low'].shift(1))               # –ù–∏–∂–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –º–∏–Ω–∏–º—É–º–∞
        ).astype(int)
        
        return df

    def _add_scalping_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–î–æ–±–∞–≤–ª—è–µ—Ç —Å–∫–∞–ª—å–ø–∏–Ω–≥–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏"""
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–∫–∞–ª—å–ø–∏–Ω–≥–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        df = self._scalping_features(df)
        
        return df

class DeepLearningModels:
    """üß† Deep Learning –º–æ–¥–µ–ª–∏ –¥–ª—è –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–æ–π —Ç–æ—Ä–≥–æ–≤–ª–∏"""
    
    def __init__(self):
        self.models = {}
        self.has_tensorflow = False
        self._initialize_models()
    
    def _initialize_models(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç CNN –∏ LSTM –º–æ–¥–µ–ª–∏"""
        try:
            import tensorflow as tf
            from tensorflow.keras.models import Sequential
            from tensorflow.keras.layers import Conv1D, LSTM, Dense, Dropout, GlobalMaxPooling1D, Attention
            
            self.has_tensorflow = True
            
            # 1D CNN –¥–ª—è price patterns
            self.models['cnn_1d'] = Sequential([
                Conv1D(64, 3, activation='relu', input_shape=(20, 5)),  # 20 —Å–≤–µ—á–µ–π, OHLCV
                Conv1D(128, 3, activation='relu'),
                GlobalMaxPooling1D(),
                Dense(64, activation='relu'),
                Dropout(0.3),
                Dense(3, activation='softmax')  # 3 –∫–ª–∞—Å—Å–∞
            ])
            
            # LSTM –¥–ª—è order flow
            self.models['lstm_flow'] = Sequential([
                LSTM(128, return_sequences=True, input_shape=(60, 3)),  # 60 —Å–≤–µ—á–µ–π, Volume+Price+Time
                LSTM(64, return_sequences=False),
                Dense(32, activation='relu'),
                Dropout(0.2),
                Dense(3, activation='softmax')
            ])
            
            # –ö–æ–º–ø–∏–ª—è—Ü–∏—è –º–æ–¥–µ–ª–µ–π
            for name, model in self.models.items():
                model.compile(
                    optimizer='adam',
                    loss='sparse_categorical_crossentropy',
                    metrics=['accuracy']
                )
            
            print(f"   ‚úÖ Deep Learning –º–æ–¥–µ–ª–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã: {list(self.models.keys())}")
            
        except ImportError:
            print("   ‚ö†Ô∏è TensorFlow –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, Deep Learning –º–æ–¥–µ–ª–∏ –ø—Ä–æ–ø—É—â–µ–Ω—ã")
    
    def prepare_sequences(self, df: pd.DataFrame, sequence_length: int = 20):
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è CNN/LSTM"""
        
        if not self.has_tensorflow:
            return None, None
        
        import numpy as np
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π  
        ohlcv = df[['open', 'high', 'low', 'close', 'volume']].values
        
        # –°–æ–∑–¥–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        sequences = []
        targets = []
        
        for i in range(sequence_length, len(ohlcv)):
            sequences.append(ohlcv[i-sequence_length:i])
            
            # Target - –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–π —Å–≤–µ—á–∏
            current_close = ohlcv[i-1, 3]  # close –ø—Ä–µ–¥—ã–¥—É—â–µ–π
            next_close = ohlcv[i, 3] if i < len(ohlcv) else current_close
            
            change = (next_close - current_close) / current_close
            
            if change > 0.002:  # >0.2% —Ä–æ—Å—Ç
                targets.append(2)  # Buy
            elif change < -0.002:  # <-0.2% –ø–∞–¥–µ–Ω–∏–µ  
                targets.append(0)  # Sell
            else:
                targets.append(1)  # Hold
        
        return np.array(sequences), np.array(targets)
    
    def train_deep_models(self, df: pd.DataFrame, epochs: int = 3):
        """–û–±—É—á–∞–µ—Ç Deep Learning –º–æ–¥–µ–ª–∏"""
        
        if not self.has_tensorflow:
            return {}
        
        results = {}
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        sequences, targets = self.prepare_sequences(df)
        
        if sequences is None:
            return {}
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
        split_idx = int(len(sequences) * 0.95)
        X_train, X_test = sequences[:split_idx], sequences[split_idx:]
        y_train, y_test = targets[:split_idx], targets[split_idx:]
        
        print(f"\nüß† –û–±—É—á–µ–Ω–∏–µ Deep Learning –º–æ–¥–µ–ª–µ–π (—É—Å–∫–æ—Ä–µ–Ω–Ω—ã–π —Ä–µ–∂–∏–º)...")
        print(f"   üìä Sequences: {len(X_train)} train, {len(X_test)} test")
        
        # –û–±—É—á–∞–µ–º CNN (–±—ã—Å—Ç—Ä–æ)
        try:
            print("   üîÑ –û–±—É—á–µ–Ω–∏–µ 1D CNN...")
            history_cnn = self.models['cnn_1d'].fit(
                X_train, y_train,
                epochs=epochs,
                batch_size=64,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º batch size –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                validation_split=0.1,
                verbose=0
            )
            
            # –û—Ü–µ–Ω–∫–∞ CNN
            loss_cnn, acc_cnn = self.models['cnn_1d'].evaluate(X_test, y_test, verbose=0)
            results['cnn_1d'] = {
                'test_accuracy': acc_cnn,
                'test_loss': loss_cnn,
                'status': 'success'
            }
            print(f"      ‚úÖ CNN: Accuracy={acc_cnn:.3f}, Loss={loss_cnn:.3f}")
            
        except Exception as e:
            print(f"      ‚ùå CNN: –û—à–∏–±–∫–∞ - {e}")
            results['cnn_1d'] = {'status': 'failed', 'error': str(e)}
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º LSTM –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (–æ–Ω –∑–∞–Ω–∏–º–∞–µ—Ç —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏)
        print("   ‚ö° LSTM –ø—Ä–æ–ø—É—â–µ–Ω –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
        results['lstm_flow'] = {'status': 'skipped', 'reason': 'Fast mode'}
        
        return results
    
    def predict_deep(self, df: pd.DataFrame):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è Deep Learning –º–æ–¥–µ–ª–µ–π"""
        
        if not self.has_tensorflow:
            return {}
        
        predictions = {}
        
        try:
            # CNN predictions
            sequences, _ = self.prepare_sequences(df.tail(100))  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 100 —Å–≤–µ—á–µ–π
            if sequences is not None and len(sequences) > 0:
                cnn_pred = self.models['cnn_1d'].predict(sequences[-10:], verbose=0)  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 10 sequences
                predictions['cnn_1d'] = np.argmax(cnn_pred, axis=1)
        except Exception as e:
            print(f"‚ö†Ô∏è CNN prediction error: {e}")
        
        try:
            # LSTM predictions (–∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ)
            # ... –∫–æ–¥ –¥–ª—è LSTM –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
            pass
        except Exception as e:
            print(f"‚ö†Ô∏è LSTM prediction error: {e}")
        
        return predictions

class TimAIModelManager:
    """ü§ñ TimAI Model Manager - –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏"""
    
    def __init__(self):
        self.models = {}
        self.model_performance = {}
        self.feature_importance = {}
        
    def initialize_models(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–æ–ª—å–∫–æ –ª—É—á—à–∏–µ –º–æ–¥–µ–ª–∏ TimAI —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
        
        print("ü§ñ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π TimAI...")
        
        # 1. XGBoost (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω) - –£–õ–£–ß–®–ï–ù–ù–´–ï –ü–ê–†–ê–ú–ï–¢–†–´
        if XGBOOST_AVAILABLE:
            try:
                self.models['xgboost'] = xgb.XGBClassifier(
                    n_estimators=500,          # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –ª—É—á—à–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏
                    max_depth=6,               # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ
                    learning_rate=0.05,        # –°–Ω–∏–∂–µ–Ω–æ –¥–ª—è –ª—É—á—à–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏
                    subsample=0.85,            # –£–ª—É—á—à–µ–Ω–æ
                    colsample_bytree=0.85,     # –£–ª—É—á—à–µ–Ω–æ
                    colsample_bylevel=0.8,     # –î–æ–±–∞–≤–ª–µ–Ω–æ
                    reg_alpha=0.1,             # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ
                    reg_lambda=1.0,            # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ
                    min_child_weight=3,        # –î–æ–±–∞–≤–ª–µ–Ω–æ
                    gamma=0.1,                 # –î–æ–±–∞–≤–ª–µ–Ω–æ
                    random_state=42,
                    eval_metric='mlogloss',
                    verbosity=0,
                    enable_categorical=False,
                    scale_pos_weight=1.0       # –î–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤
                )
            except Exception as e:
                print(f"   ‚ö†Ô∏è XGBoost –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å –æ—à–∏–±–∫–æ–π: {e}")
                self.models['xgboost'] = xgb.XGBClassifier(
                    n_estimators=300,
                    max_depth=6,
                    learning_rate=0.1,
                    random_state=42,
                    verbosity=0
                )

        # 2. LightGBM (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω) - –£–õ–£–ß–®–ï–ù–ù–´–ï –ü–ê–†–ê–ú–ï–¢–†–´
        if LIGHTGBM_AVAILABLE:
            self.models['lightgbm'] = lgb.LGBMClassifier(
                n_estimators=500,              # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –ª—É—á—à–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏
                max_depth=6,                   # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ
                learning_rate=0.05,            # –°–Ω–∏–∂–µ–Ω–æ –¥–ª—è –ª—É—á—à–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏
                subsample=0.85,                # –î–æ–±–∞–≤–ª–µ–Ω–æ
                colsample_bytree=0.85,         # –î–æ–±–∞–≤–ª–µ–Ω–æ
                reg_alpha=0.1,                 # –î–æ–±–∞–≤–ª–µ–Ω–æ
                reg_lambda=1.0,                # –î–æ–±–∞–≤–ª–µ–Ω–æ
                min_child_samples=20,          # –î–æ–±–∞–≤–ª–µ–Ω–æ
                min_split_gain=0.1,            # –î–æ–±–∞–≤–ª–µ–Ω–æ
                random_state=42,
                verbose=-1,
                class_weight='balanced'        # –î–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤
            )

        print(f"   ‚úÖ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(self.models)} –º–æ–¥–µ–ª–µ–π (–º–∏–Ω–∏–º–∞–ª–∏–∑–º - —Ç–æ–ª—å–∫–æ –ª—É—á—à–∏–µ)")
        
    def train_all_models(self, X: pd.DataFrame, y: np.ndarray) -> Dict[str, Dict]:
        """–û–±—É—á–∞–µ—Ç –≤—Å–µ –º–æ–¥–µ–ª–∏ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π –æ–±—É—á–µ–Ω–∏—è"""
        
        print(f"\nüöÄ TimAI: –û–±—É—á–µ–Ω–∏–µ {len(self.models)} –º–æ–¥–µ–ª–µ–π...")
        
        results = {}
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤
        class_counts = np.bincount(y)
        print(f"   üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {class_counts}")
        
        # –í—ã—á–∏—Å–ª—è–µ–º class_weight –¥–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        total_samples = len(y)
        class_weights = {i: total_samples / (len(class_counts) * count) for i, count in enumerate(class_counts)}
        print(f"   ‚öñÔ∏è Class weights: {class_weights}")
        
        for name, model in self.models.items():
            print(f"   üîÑ –û–±—É—á–µ–Ω–∏–µ {name}...")

            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏
            X_train = X.astype(np.float32) if name == 'xgboost' else X

            start_time = time.time()

            try:
                # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º class_weight –¥–ª—è –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ —ç—Ç–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç
                if hasattr(model, 'class_weight') and model.class_weight is None:
                    try:
                        model.set_params(class_weight=class_weights)
                    except:
                        pass  # –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –º–æ–¥–µ–ª–∏ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç class_weight
                
                # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
                model.fit(X_train, y)
                
                # –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è
                training_time = time.time() - start_time
                
                # –£–õ–£–ß–®–ï–ù–ù–ê–Ø Cross-validation —Å StratifiedKFold
                try:
                    from sklearn.model_selection import StratifiedKFold
                    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
                    cv_scores = cross_val_score(model, X_train, y, cv=skf, scoring='f1_weighted')
                    
                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
                    cv_accuracy = cross_val_score(model, X_train, y, cv=skf, scoring='accuracy')
                    cv_precision = cross_val_score(model, X_train, y, cv=skf, scoring='precision_weighted')
                    cv_recall = cross_val_score(model, X_train, y, cv=skf, scoring='recall_weighted')
                    
                except Exception as cv_error:
                    print(f"      ‚ö†Ô∏è {name}: CV –æ—à–∏–±–∫–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—É—é –æ—Ü–µ–Ω–∫—É: {cv_error}")
                    # –ü—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                    pred = model.predict(X_train)
                    from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score
                    f1 = f1_score(y, pred, average='weighted')
                    accuracy = accuracy_score(y, pred)
                    precision = precision_score(y, pred, average='weighted')
                    recall = recall_score(y, pred, average='weighted')
                    cv_scores = np.array([f1, f1, f1])
                    cv_accuracy = np.array([accuracy, accuracy, accuracy])
                    cv_precision = np.array([precision, precision, precision])
                    cv_recall = np.array([recall, recall, recall])
                
                # Feature importance (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)
                if hasattr(model, 'feature_importances_'):
                    self.feature_importance[name] = dict(zip(X.columns, model.feature_importances_))
                
                results[name] = {
                    'cv_f1_mean': cv_scores.mean(),
                    'cv_f1_std': cv_scores.std(),
                    'cv_accuracy_mean': cv_accuracy.mean(),
                    'cv_precision_mean': cv_precision.mean(),
                    'cv_recall_mean': cv_recall.mean(),
                    'training_time': training_time,
                    'status': 'success'
                }
                
                print(f"      ‚úÖ {name}: F1={cv_scores.mean():.3f}¬±{cv_scores.std():.3f}, "
                      f"Acc={cv_accuracy.mean():.3f}, Prec={cv_precision.mean():.3f}, "
                      f"Rec={cv_recall.mean():.3f}, Time={training_time:.1f}s")
                
            except Exception as e:
                print(f"      ‚ùå {name}: –û—à–∏–±–∫–∞ - {str(e)}")
                results[name] = {
                    'status': 'failed',
                    'error': str(e)
                }
        
        self.model_performance = results
        return results
    
    def evaluate_on_test(self, X_test: pd.DataFrame, y_test: np.ndarray) -> Dict[str, Dict]:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        
        print(f"\nüìä –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        
        from sklearn.metrics import f1_score, accuracy_score, classification_report
        
        test_results = {}
        
        for name, model in self.models.items():
            if name in self.model_performance and self.model_performance[name]['status'] == 'success':
                try:
                    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ —Ç–µ—Å—Ç–µ
                    y_pred = model.predict(X_test)
                    
                    # –ú–µ—Ç—Ä–∏–∫–∏
                    f1 = f1_score(y_test, y_pred, average='weighted')
                    accuracy = accuracy_score(y_test, y_pred)
                    
                    test_results[name] = {
                        'f1_score': f1,
                        'accuracy': accuracy,
                        'status': 'success'
                    }
                    
                    print(f"   ‚úÖ {name}: F1={f1:.3f}, Accuracy={accuracy:.3f}")
                    
                except Exception as e:
                    print(f"   ‚ùå {name}: –û—à–∏–±–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–µ - {str(e)}")
                    test_results[name] = {
                        'status': 'failed',
                        'error': str(e)
                    }
        
        return test_results
    
    def predict_ensemble(self, X: pd.DataFrame, method: str = 'weighted_voting') -> Tuple[np.ndarray, Dict]:
        """–ê–Ω—Å–∞–º–±–ª–µ–≤–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
        
        predictions = {}
        weights = {}
        
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ç –≤—Å–µ—Ö —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        for name, model in self.models.items():
            if name in self.model_performance and self.model_performance[name]['status'] == 'success':
                try:
                    pred = model.predict(X)
                    # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —ç—Ç–æ —Å–∫–∞–ª—è—Ä—ã, –∞ –Ω–µ –º–∞—Å—Å–∏–≤—ã
                    if hasattr(pred, 'flatten'):
                        pred = pred.flatten()
                    predictions[name] = pred
                    
                    # –í–µ—Å –Ω–∞ –æ—Å–Ω–æ–≤–µ CV F1-score
                    weights[name] = self.model_performance[name]['cv_f1_mean']
                    
                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è {name}: {e}")
        
        if not predictions:
            return np.zeros(len(X)), {}
        
        # –ú–µ—Ç–æ–¥—ã –∞–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è
        if method == 'simple_voting':
            # –ü—Ä–æ—Å—Ç–æ–µ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ
            ensemble_pred = self._simple_voting(predictions)
        elif method == 'weighted_voting':
            # –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ
            ensemble_pred = self._weighted_voting(predictions, weights)
        else:
            # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é - –≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ
            ensemble_pred = self._weighted_voting(predictions, weights)
        
        return ensemble_pred, predictions
    
    def _simple_voting(self, predictions: Dict[str, np.ndarray]) -> np.ndarray:
        """–ü—Ä–æ—Å—Ç–æ–µ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ"""
        
        pred_matrix = np.array(list(predictions.values()))
        ensemble_pred = []
        
        for i in range(pred_matrix.shape[1]):
            votes = {}
            for pred in pred_matrix[:, i]:
                votes[pred] = votes.get(pred, 0) + 1
            
            best_pred = max(votes, key=votes.get)
            ensemble_pred.append(best_pred)
        
        return np.array(ensemble_pred)
    
    def _weighted_voting(self, predictions: Dict[str, np.ndarray], weights: Dict[str, float]) -> np.ndarray:
        """–í–∑–≤–µ—à–µ–Ω–Ω–æ–µ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ"""
        
        ensemble_pred = []
        weight_sum = sum(weights.values())
        
        if weight_sum == 0:
            return self._simple_voting(predictions)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–µ—Å–∞
        normalized_weights = {name: w/weight_sum for name, w in weights.items()}
        
        for i in range(len(list(predictions.values())[0])):
            votes = {}
            
            for name, pred_array in predictions.items():
                if name in normalized_weights:
                    # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–∫–∞–ª—è—Ä–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è
                    pred = pred_array[i]
                    if hasattr(pred, 'item'):  # numpy scalar
                        pred = pred.item()
                    elif isinstance(pred, (list, tuple)):  # –º–∞—Å—Å–∏–≤
                        pred = pred[0] if len(pred) > 0 else 0
                    
                    # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ pred —ç—Ç–æ hashable —Ç–∏–ø
                    pred = int(pred) if not isinstance(pred, (int, float, str)) else pred
                    
                    votes[pred] = votes.get(pred, 0) + normalized_weights[name]
            
            best_pred = max(votes, key=votes.get) if votes else 0
            ensemble_pred.append(best_pred)
        
        return np.array(ensemble_pred)
    
    def save_models(self, filepath: str = "models/production"):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—Å–µ –º–æ–¥–µ–ª–∏ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
        
        os.makedirs(filepath, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        saved_models = []
        
        for name, model in self.models.items():
            if name in self.model_performance and self.model_performance[name]['status'] == 'success':
                # –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏
                model_path = os.path.join(filepath, f"timai_{name}_{timestamp}.pkl")
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
                joblib.dump(model, model_path)
                
                # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                metadata = {
                    'model_name': name,
                    'timestamp': timestamp,
                    'performance': self.model_performance[name],
                    'feature_importance': self.feature_importance.get(name, {}),
                    'file_size_mb': os.path.getsize(model_path) / 1024 / 1024
                }
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                meta_path = os.path.join(filepath, f"timai_{name}_{timestamp}_meta.json")
                with open(meta_path, 'w') as f:
                    json.dump(metadata, f, indent=2, default=str)
                
                saved_models.append({
                    'name': name,
                    'model_path': model_path,
                    'meta_path': meta_path,
                    'size_mb': metadata['file_size_mb']
                })
                
                print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –º–æ–¥–µ–ª—å {name}: {metadata['file_size_mb']:.1f} MB")
        
        return saved_models

class TimAI:
    """ü§ñ TimAI - –ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å —Ç–æ—Ä–≥–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã"""
    
    def __init__(self):
        self.feature_engine = TimAIFeatureEngine()
        self.model_manager = TimAIModelManager()
        self.deep_models = DeepLearningModels()  # –î–æ–±–∞–≤–ª—è–µ–º Deep Learning
        self.is_trained = False
        
    def prepare_data(self, df: pd.DataFrame, balance_method=None) -> Tuple[pd.DataFrame, np.ndarray]:
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∏ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π –∫–ª–∞—Å—Å–æ–≤"""
        
        print("üìä TimAI: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        
        # Feature engineering
        df_features = self.feature_engine.engineer_features(df.copy())
        
        # –£–õ–£–ß–®–ï–ù–ù–ê–Ø –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è (3-–∫–ª–∞—Å—Å–æ–≤–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è)
        returns = df_features['close'].pct_change().shift(-1)
        vol = returns.rolling(50).std()
        
        # –ë–æ–ª–µ–µ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        target = np.where(returns > vol * 0.6, 2,      # Buy (—É–ª—É—á—à–µ–Ω–æ)
                         np.where(returns > vol * 0.2, 1,      # Hold (—É–ª—É—á—à–µ–Ω–æ)
                                np.where(returns < -vol * 0.6, 0,   # Sell (—É–ª—É—á—à–µ–Ω–æ)
                                       1)))  # Hold (default)
        
        # –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        df_features = df_features.iloc[:-1]  # –£–±–∏—Ä–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Å—Ç—Ä–æ–∫—É –∏–∑-–∑–∞ shift
        target = target[:-1]
        
        # –£–¥–∞–ª—è–µ–º –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        valid_mask = ~(np.isnan(target) | np.isinf(target))
        df_features = df_features[valid_mask]
        target = target[valid_mask]
        
        # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        exclude_cols = ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'turnover', 'datetime']
        numeric_features = df_features.select_dtypes(include=[np.number]).columns
        feature_cols = [col for col in numeric_features if col not in exclude_cols]
        
        X = df_features[feature_cols]
        
        # –£–õ–£–ß–®–ï–ù–ù–ê–Ø –û–ß–ò–°–¢–ö–ê –ü–†–ò–ó–ù–ê–ö–û–í
        # –£–¥–∞–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –Ω—É–ª–µ–≤–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π
        from sklearn.feature_selection import VarianceThreshold
        selector = VarianceThreshold(threshold=0.01)
        X_cleaned = selector.fit_transform(X)
        selected_features = X.columns[selector.get_support()].tolist()
        X = X[selected_features]
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–µ–ª–µ–∫—Ç–æ—Ä –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏
        self.feature_selector = selector
        self.selected_features = selected_features
        
        print(f"   ‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ: {len(X)} –æ–±—Ä–∞–∑—Ü–æ–≤, {len(X.columns)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        print(f"   üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {np.bincount(target)}")
        
        # –ê–Ω–∞–ª–∏–∑ –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏
        class_counts = np.bincount(target)
        imbalance_ratio = max(class_counts) / min(class_counts)
        print(f"   ‚öñÔ∏è –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {imbalance_ratio:.2f}:1")
        
        # –£–õ–£–ß–®–ï–ù–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê –ù–ï–°–ë–ê–õ–ê–ù–°–ò–†–û–í–ê–ù–ù–´–• –î–ê–ù–ù–´–•
        if imbalance_ratio > 3.0:  # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ —Å–∏–ª—å–Ω–æ –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω—ã
            print(f"   üîß –ü—Ä–∏–º–µ–Ω—è–µ–º SMOTE –¥–ª—è —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö...")
            try:
                from imblearn.over_sampling import SMOTE
                from imblearn.under_sampling import RandomUnderSampler
                from imblearn.pipeline import Pipeline
                
                # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º SMOTE –∏ RandomUnderSampler
                over = SMOTE(sampling_strategy=0.5, random_state=42)
                under = RandomUnderSampler(sampling_strategy=0.8, random_state=42)
                steps = [('o', over), ('u', under)]
                pipeline = Pipeline(steps=steps)
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏–µ
                X_resampled, y_resampled = pipeline.fit_resample(X, target)
                
                print(f"   ‚úÖ –ü–æ—Å–ª–µ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏—è: {len(X_resampled)} –æ–±—Ä–∞–∑—Ü–æ–≤")
                print(f"   üìä –ù–æ–≤–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: {np.bincount(y_resampled)}")
                
                return X_resampled, y_resampled
                
            except ImportError:
                print(f"   ‚ö†Ô∏è imbalanced-learn –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
                return X, target
        else:
            print(f"   ‚úÖ –î–∞–Ω–Ω—ã–µ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω—ã")
            return X, target
        
        # --- –î–û–ë–ê–í–õ–ï–ù–û: –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤ —á–µ—Ä–µ–∑ SMOTE/Over/Under ---
        if balance_method and balance_data is not None:
            print(f"   üîß –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤ –º–µ—Ç–æ–¥–æ–º: {balance_method}")
            X_bal, y_bal = balance_data(X, target, method=balance_method)
            print(f"   ‚úÖ –ü–æ—Å–ª–µ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏: {len(X_bal)} –æ–±—Ä–∞–∑—Ü–æ–≤, –∫–ª–∞—Å—Å—ã: {np.bincount(y_bal)}")
            return X_bal, y_bal
    
    def train(self, df: pd.DataFrame, balance_method=None, optimize_hyperparams=False, model_type='xgboost'):
        """–û–±—É—á–∞–µ—Ç –≤—Å–µ –º–æ–¥–µ–ª–∏ TimAI —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤ –∏ –≥–∏–ø–µ—Ä–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
        print("üöÄ TimAI: –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã...")
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X, y = self.prepare_data(df, balance_method=balance_method)
        self.feature_cols = X.columns.tolist()
        print(f"   üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(self.feature_cols)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è")
        # --- –î–û–ë–ê–í–õ–ï–ù–û: –ì–∏–ø–µ—Ä–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ Optuna ---
        best_params = None
        if optimize_hyperparams and optimize_model is not None:
            print(f"   üî¨ –ó–∞–ø—É—Å–∫ Optuna –¥–ª—è {model_type}...")
            best_params = optimize_model(X, y, model_type=model_type)
            print(f"   üèÜ –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã Optuna: {best_params}")
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test (95%/5%)
        from sklearn.model_selection import train_test_split
        split_idx = int(len(X) * 0.95)
        
        X_train = X.iloc[:split_idx]
        X_test = X.iloc[split_idx:]
        y_train = y[:split_idx]
        y_test = y[split_idx:]
        
        print(f"   üìä Train: {len(X_train):,} –æ–±—Ä–∞–∑—Ü–æ–≤, Test: {len(X_test):,} –æ–±—Ä–∞–∑—Ü–æ–≤")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
        self.model_manager.initialize_models()
        
        # –û–±—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ train –¥–∞–Ω–Ω—ã—Ö
        results = self.model_manager.train_all_models(X_train, y_train)
        
        # –û–±—É—á–µ–Ω–∏–µ Deep Learning –º–æ–¥–µ–ª–µ–π
        deep_results = self.deep_models.train_deep_models(df.iloc[:split_idx])  # –ù–∞ —Ç–µ—Ö –∂–µ train –¥–∞–Ω–Ω—ã—Ö
        
        # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ test –¥–∞–Ω–Ω—ã—Ö
        test_results = self.model_manager.evaluate_on_test(X_test, y_test)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
        saved_models = self.model_manager.save_models()
        
        self.is_trained = True
        
        print(f"\nüéâ TimAI: –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print(f"   ü§ñ –û–±—É—á–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: {len([r for r in results.values() if r['status'] == 'success'])}")
        print(f"   üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: {len(saved_models)}")
        
        # –¢–æ–ø-3 –º–æ–¥–µ–ª–∏ –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ train
        successful_models = {name: res for name, res in results.items() if res['status'] == 'success'}
        if successful_models:
            top_models = sorted(successful_models.items(), key=lambda x: x[1]['cv_f1_mean'], reverse=True)[:3]
            
            print(f"\nüèÜ –¢–æ–ø-3 –º–æ–¥–µ–ª–∏ –ø–æ F1-score (Train):")
            for i, (name, res) in enumerate(top_models, 1):
                print(f"   {i}. {name}: {res['cv_f1_mean']:.3f}¬±{res['cv_f1_std']:.3f}")
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ test
        if test_results:
            print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ Test –¥–∞–Ω–Ω—ã—Ö:")
            for name, result in test_results.items():
                if 'f1_score' in result:
                    print(f"   {name}: F1={result['f1_score']:.3f}, Accuracy={result['accuracy']:.3f}")
        
        return results
    
    def predict(self, df: pd.DataFrame, method: str = 'weighted_voting', confidence_threshold: float = 0.5) -> Tuple[np.ndarray, Dict]:
        """–î–µ–ª–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º meta-—Ñ–∏–ª—å—Ç—Ä–æ–º –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –≤–∏–Ω—Ä–µ–π—Ç–∞"""
        if not self.is_trained:
            raise ValueError("TimAI –Ω–µ –æ–±—É—á–µ–Ω–∞! –°–Ω–∞—á–∞–ª–∞ –≤—ã–∑–æ–≤–∏—Ç–µ train()")
        
        df_features = self.feature_engine.engineer_features(df.copy())
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ –ø—Ä–∏–∑–Ω–∞–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏
        if hasattr(self, 'feature_cols'):
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å–µ –Ω—É–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –µ—Å—Ç—å
            missing_features = [col for col in self.feature_cols if col not in df_features.columns]
            if missing_features:
                print(f"   ‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏: {missing_features[:5]}...")
                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∫–∞–∫ –Ω—É–ª–∏
                for col in missing_features:
                    df_features[col] = 0
            
            X = df_features[self.feature_cols]
            print(f"   ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º {len(self.feature_cols)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è")
        else:
            # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ —á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            exclude_cols = ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'turnover', 'datetime']
            numeric_features = df_features.select_dtypes(include=[np.number]).columns
            feature_cols = [col for col in numeric_features if col not in exclude_cols]
            X = df_features[feature_cols]
            print(f"   ‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (fallback): {len(feature_cols)}")
        
        # –£–õ–£–ß–®–ï–ù–ù–´–ô Meta-—Ñ–∏–ª—å—Ç—Ä —Å –±–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π
        preds = {}
        confidences = {}
        probabilities = {}
        
        for name, model in self.model_manager.models.items():
            if name in self.model_manager.model_performance and self.model_manager.model_performance[name]['status'] == 'success':
                try:
                    if hasattr(model, 'predict_proba'):
                        proba = model.predict_proba(X)
                        conf = np.max(proba, axis=1)
                        pred = np.argmax(proba, axis=1)
                        preds[name] = pred
                        confidences[name] = conf
                        probabilities[name] = proba
                    else:
                        pred = model.predict(X)
                        preds[name] = pred
                        confidences[name] = np.ones(len(pred)) * 0.5
                        probabilities[name] = None
                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è {name}: {e}")
                    continue
        
        if not preds:
            return np.array([1] * len(X)), {}  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º Hold –µ—Å–ª–∏ –Ω–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        
        # –£–õ–£–ß–®–ï–ù–ù–ê–Ø –õ–û–ì–ò–ö–ê –ê–ù–°–ê–ú–ë–õ–ò–†–û–í–ê–ù–ò–Ø
        meta_signal = []
        
        for i in range(len(X)):
            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è —Ç–µ–∫—É—â–µ–π —Ç–æ—á–∫–∏
            current_preds = {}
            current_confs = {}
            
            for name in preds:
                current_preds[name] = preds[name][i]
                current_confs[name] = confidences[name][i]
            
            # –£–õ–£–ß–®–ï–ù–ù–´–ô –ê–õ–ì–û–†–ò–¢–ú –ü–†–ò–ù–Ø–¢–ò–Ø –†–ï–®–ï–ù–ò–ô
            
            # 1. –ù–∞—Ö–æ–¥–∏–º –º–æ–¥–µ–ª–∏ —Å –≤—ã—Å–æ–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é
            high_conf_models = {name: conf for name, conf in current_confs.items() if conf >= confidence_threshold}
            
            # 2. –ï—Å–ª–∏ –µ—Å—Ç—å –º–æ–¥–µ–ª–∏ —Å –≤—ã—Å–æ–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é
            if len(high_conf_models) >= 2:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≥–ª–∞—Å–∏–µ –º–µ–∂–¥—É –≤—ã—Å–æ–∫–æ—É–≤–µ—Ä–µ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏
                high_conf_preds = [current_preds[name] for name in high_conf_models]
                
                if len(set(high_conf_preds)) == 1:  # –í—Å–µ –º–æ–¥–µ–ª–∏ —Å–æ–≥–ª–∞—Å–Ω—ã
                    # –°–∏–ª—å–Ω—ã–π —Å–∏–≥–Ω–∞–ª - –≤—Å–µ –º–æ–¥–µ–ª–∏ —Å–æ–≥–ª–∞—Å–Ω—ã –∏ —É–≤–µ—Ä–µ–Ω—ã
                    meta_signal.append(high_conf_preds[0])
                else:
                    # –ú–æ–¥–µ–ª–∏ –Ω–µ —Å–æ–≥–ª–∞—Å–Ω—ã - –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ
                    weighted_votes = {}
                    for name in high_conf_models:
                        pred = current_preds[name]
                        conf = current_confs[name]
                        weighted_votes[pred] = weighted_votes.get(pred, 0) + conf
                    
                    best_pred = max(weighted_votes, key=weighted_votes.get)
                    meta_signal.append(best_pred)
                    
            elif len(high_conf_models) == 1:
                # –¢–æ–ª—å–∫–æ –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å —É–≤–µ—Ä–µ–Ω–∞ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ—ë —Å–∏–≥–Ω–∞–ª
                confident_model = list(high_conf_models.keys())[0]
                meta_signal.append(current_preds[confident_model])
                
            else:
                # –ù–µ—Ç –≤—ã—Å–æ–∫–æ—É–≤–µ—Ä–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π - –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω–æ–µ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ
                # –í–∑–≤–µ—à–∏–≤–∞–µ–º –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –∏ –∫–∞—á–µ—Å—Ç–≤—É –º–æ–¥–µ–ª–∏
                weighted_votes = {}
                
                for name in current_preds:
                    pred = current_preds[name]
                    conf = current_confs[name]
                    
                    # –ü–æ–ª—É—á–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è
                    model_quality = self.model_manager.model_performance[name].get('cv_f1_mean', 0.5)
                    
                    # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤–µ—Å: —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å * –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏
                    weight = conf * model_quality
                    weighted_votes[pred] = weighted_votes.get(pred, 0) + weight
                
                if weighted_votes:
                    best_pred = max(weighted_votes, key=weighted_votes.get)
                    meta_signal.append(best_pred)
                else:
                    meta_signal.append(1)  # Hold –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        
        return np.array(meta_signal), preds

    def predict_single_model(self, df: pd.DataFrame, model_name: str):
        """–ë—ã—Å—Ç—Ä–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –æ–¥–Ω–æ–π –º–æ–¥–µ–ª—å—é –¥–ª—è —Å–∫–∞–ª—å–ø–∏–Ω–≥–∞"""
        
        if not self.is_trained:
            return None
        
        try:
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            X, _ = self.prepare_data(df)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –æ–¥–Ω–æ–π –º–æ–¥–µ–ª—å—é
            if model_name in self.model_manager.models:
                model = self.model_manager.models[model_name]
                prediction = model.predict(X.tail(1))[0]  # –¢–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω—è—è —Å–≤–µ—á–∞
                return int(prediction)
            
            return None
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ predict_single_model {model_name}: {e}")
            return None

    def save_model(self, model_path: str = None):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å TimAI"""

def main():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã TimAI"""
    
    print("ü§ñ TimAI - Advanced Multi-Model Trading System")
    print("="*60)
    
    print(f"\nüìã –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏:")
    print(f"   üéØ XGBoost: {'‚úÖ' if XGBOOST_AVAILABLE else '‚ùå'}")
    print(f"   ‚ö° LightGBM: {'‚úÖ' if LIGHTGBM_AVAILABLE else '‚ùå'}")
    print(f"   üîç Optuna: {'‚úÖ' if OPTUNA_AVAILABLE else '‚ùå'}")
    print(f"   üå≤ Random Forest: ‚úÖ")
    print(f"   üìà Incremental Learning: ‚úÖ")
    print(f"   üß† Neural Networks: ‚úÖ")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è TimAI
    timai = TimAI()
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    try:
        df = pd.read_csv('data/historical/BTCUSDT_15m_5years_20210705_20250702.csv')
        print(f"\nüìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {len(df)} –∑–∞–ø–∏—Å–µ–π BTCUSDT 15m")
    except FileNotFoundError:
        print("‚ùå –§–∞–π–ª –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω! –°–æ–∑–¥–∞–π—Ç–µ –ø–∞–ø–∫—É data/historical/ –∏ –ø–æ–º–µ—Å—Ç–∏—Ç–µ —Ç—É–¥–∞ –¥–∞–Ω–Ω—ã–µ")
        return None
    
    # –û–±—É—á–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã
    start_time = time.time()
    results = timai.train(df)
    total_time = time.time() - start_time
    
    print(f"\n‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {total_time:.1f} —Å–µ–∫—É–Ω–¥")
    
    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    if timai.is_trained:
        print(f"\nüîÆ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 100 —Å–≤–µ—á–∞—Ö...")
        test_data = df.tail(100)
        predictions, individual_preds = timai.predict(test_data)
        
        # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        predictions_clean = [max(0, int(p)) for p in predictions]
        print(f"   üìä –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {np.bincount(predictions_clean)}")
        print(f"   ü§ñ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –º–æ–¥–µ–ª–µ–π: {len(individual_preds)}")
    
    print(f"\nüéâ TimAI –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ!")
    
    return timai

if __name__ == "__main__":
    timai = main() 