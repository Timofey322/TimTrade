#!/usr/bin/env python3
"""
–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π Smart Adaptive Trading Pipeline
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–æ—Ç–æ–≤—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∏–∑ JSON —Ñ–∞–π–ª–æ–≤ –±–µ–∑ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
"""

import os
import sys
import json
import logging
from pathlib import Path
import pandas as pd
import numpy as np
from datetime import datetime
import yaml

# –î–æ–±–∞–≤–ª—è–µ–º src –≤ path
sys.path.append(str(Path(__file__).parent / 'src'))

from data_collection.collector import DataCollector
from preprocessing.feature_engineering import FeatureEngineer
from preprocessing.multi_timeframe import MultiTimeframeProcessor
from ml_models.advanced_xgboost_model import AdvancedEnsembleModel

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(name)s | %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('logs/smart_adaptive.log', encoding='utf-8')
    ]
)

def setup_logger():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞"""
    logger = logging.getLogger(__name__)
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –ª–æ–≥–æ–≤
    Path('logs').mkdir(exist_ok=True)
    return logger

def load_config(config_path: str) -> dict:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ YAML —Ñ–∞–π–ª–∞"""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    except Exception as e:
        logging.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é {config_path}: {e}")
        return {}

def load_recommendations(timeframe: str) -> dict:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞"""
    filename = f"recommendations_{timeframe}_3years.json"
    if os.path.exists(filename):
        with open(filename, 'r', encoding='utf-8') as f:
            return json.load(f)
    else:
        # –î–µ—Ñ–æ–ª—Ç–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –µ—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω
        return {
            "best_threshold": 0.001 if timeframe == "5m" else 0.0015 if timeframe == "15m" else 0.003,
            "top_features": ["sma_20", "rsi", "macd", "bb_upper", "bb_lower", "volume"],
            "best_params": {
                "n_estimators": 300,
                "max_depth": 8,
                "learning_rate": 0.02
            }
        }

def get_best_indicators(timeframe: str) -> list:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ª—É—á—à–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –¥–ª—è —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"""
    # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ª—É—á—à–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞—à–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π
    indicator_mapping = {
        "5m": ["macd", "obv", "vwap"],
        "15m": ["rsi", "obv", "vwap"], 
        "1h": ["rsi", "obv", "vwap"]
    }
    return indicator_mapping.get(timeframe, ["macd", "rsi", "bollinger"])

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è pipeline"""
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    logger = setup_logger()
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ —É–ø—Ä–æ—â–µ–Ω–Ω–æ–≥–æ Smart Adaptive Pipeline")
    
    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ (—Ç–µ–ø–µ—Ä—å —Å enhanced –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏)
        config = load_config('config/smart_adaptive_config.yaml')
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
        data_config = config.get('data_collection', {})
        symbols = data_config.get('symbols', ['BTC/USDT'])
        timeframes = data_config.get('timeframes', ['5m', '15m', '1h'])
        
        # === –°–ë–û–† –î–ê–ù–ù–´–• ===
        logger.info("=== –°–ë–û–† –î–ê–ù–ù–´–• ===")
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è DataCollector
        collector_config = {
            'exchange': 'binance',
            'symbols': symbols,
            'timeframes': timeframes,
            'primary_timeframe': timeframes[0],
            'limits': {tf: 200000 for tf in timeframes}
        }
        collector = DataCollector(collector_config)
        
        all_data = {}
        for symbol in symbols:
            symbol_data = {}
            for timeframe in timeframes:
                logger.info(f"–°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol} –Ω–∞ {timeframe}")
                data = collector.fetch_ohlcv_with_history(
                    symbol=symbol,
                    timeframe=timeframe,
                    target_limit=200000  # –ú–∞–∫—Å–∏–º—É–º –¥–∞–Ω–Ω—ã—Ö
                )
                if data is not None and not data.empty:
                    symbol_data[timeframe] = data
                    logger.info(f"–°–æ–±—Ä–∞–Ω–æ {len(data)} –∑–∞–ø–∏—Å–µ–π –¥–ª—è {timeframe}")
                else:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è {symbol} –Ω–∞ {timeframe}")
            
            if symbol_data:
                all_data[symbol] = symbol_data
        
        # === –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê –î–ê–ù–ù–´–• ===
        logger.info("=== –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê –î–ê–ù–ù–´–• ===")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤
        multi_config = {
            'enabled': True,
            'primary_timeframe': timeframes[0],
            'features_per_timeframe': {tf: [] for tf in timeframes},
            'prefixes': {tf: f"{tf}_" for tf in timeframes if tf != timeframes[0]}
        }
        multi_processor = MultiTimeframeProcessor(multi_config)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å enhanced –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
        feature_engineer = FeatureEngineer(config)
        
        processed_data = {}
        
        for symbol in symbols:
            logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}")
            symbol_clean = symbol.replace('/', '_')
            
            if len(timeframes) > 1:
                logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ–º –º—É–ª—å—Ç–∏-—Ç–∞–π–º—Ñ—Ä–µ–π–º –æ–±—Ä–∞–±–æ—Ç–∫—É")
                
                # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è
                if not multi_processor.validate_timeframe_data(all_data[symbol]):
                    logger.error(f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}")
                    continue
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∂–¥–æ–≥–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞ —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º–∏
                tf_data = {}
                for timeframe in timeframes:
                    logger.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–∞–π–º—Ñ—Ä–µ–π–º {timeframe}")
                    
                    # –ü–æ–ª—É—á–∞–µ–º –ª—É—á—à–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –¥–ª—è —ç—Ç–æ–≥–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞
                    best_indicators = get_best_indicators(timeframe)
                    logger.info(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã: {best_indicators}")
                    
                    # –ì–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
                    horizon_map = {"5m": 12, "15m": 10, "1h": 8}
                    horizon = horizon_map.get(timeframe, 12)
                    
                    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º–∏
                    processed = feature_engineer.process_features(
                        data=all_data[symbol][timeframe],
                        symbol=symbol_clean,
                        timeframe=timeframe,
                        prediction_horizon=horizon,
                        adaptive_indicators_enabled=False,  # –û—Ç–∫–ª—é—á–∞–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é
                        fixed_indicators=best_indicators    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
                    )
                    
                    tf_data[timeframe] = processed
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ sentiment —Ñ–∏—á–µ–π
                    sentiment_cols = [col for col in processed.columns if any(
                        keyword in col.lower() for keyword in ['sentiment', 'bybit', 'regime']
                    )]
                    
                    logger.info(f"–¢–∞–π–º—Ñ—Ä–µ–π–º {timeframe}: {len(processed)} —Å—Ç—Ä–æ–∫, {len(processed.columns)} –∫–æ–ª–æ–Ω–æ–∫")
                    if sentiment_cols:
                        logger.info(f"‚úÖ Enhanced: –Ω–∞–π–¥–µ–Ω–æ {len(sentiment_cols)} sentiment —Ñ–∏—á–µ–π")
                        
                        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–µ–∫—É—â–∏–π market regime
                        if 'market_regime_bullish' in processed.columns:
                            regime = "üêÇ –ë–´–ß–ò–ô" if processed['market_regime_bullish'].iloc[-1] else \
                                    "üêª –ú–ï–î–í–ï–ñ–ò–ô" if processed['market_regime_bearish'].iloc[-1] else \
                                    "üòê –ù–ï–ô–¢–†–ê–õ–¨–ù–´–ô"
                            sentiment_score = processed.get('sentiment_composite_score', pd.Series([0.5])).iloc[-1]
                            logger.info(f"üìä Market Regime: {regime}, Sentiment: {sentiment_score:.3f}")
                    else:
                        logger.info("‚ÑπÔ∏è Sentiment —Ñ–∏—á–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã (–∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Ç–æ–ª—å–∫–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã)")
                
                processed_data[symbol] = tf_data
            else:
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞
                timeframe = timeframes[0]
                best_indicators = get_best_indicators(timeframe)
                logger.info(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã: {best_indicators}")
                
                processed = feature_engineer.process_features(
                    data=all_data[symbol][timeframe],
                    symbol=symbol_clean,
                    timeframe=timeframe,
                    adaptive_indicators_enabled=False,
                    fixed_indicators=best_indicators
                )
                processed_data[symbol] = {timeframe: processed}
        
        # === –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô ===
        logger.info("=== –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô ===")
        
        models = {}
        for symbol in symbols:
            symbol_clean = symbol.replace('/', '_')
            symbol_models = {}
            
            for timeframe in timeframes:
                logger.info(f"–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è {symbol} –Ω–∞ {timeframe}")
                
                data = processed_data[symbol][timeframe]
                if len(data) < 1000:
                    logger.warning(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol} {timeframe}: {len(data)}")
                    continue
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
                timeframe_config = config.get('timeframe_params', {}).get(timeframe, {})
                
                # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
                xgb_params = timeframe_config.get("xgboost", {
                    "n_estimators": 300,
                    "max_depth": 8,
                    "learning_rate": 0.02,
                    "random_state": 42,
                    "n_jobs": -1
                })
                
                # –û–±—É—á–µ–Ω–∏–µ
                try:
                    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
                    model = AdvancedEnsembleModel(config={
                        'timeframe_params': {
                            timeframe: {
                                'xgboost': xgb_params,
                                'random_forest': timeframe_config.get('random_forest', {
                                    'n_estimators': 200,
                                    'max_depth': 10,
                                    'random_state': 42,
                                    'n_jobs': -1
                                }),
                                'lightgbm': timeframe_config.get('lightgbm', {
                                    'n_estimators': 300,
                                    'max_depth': 8,
                                    'learning_rate': 0.02,
                                    'random_state': 42,
                                    'n_jobs': -1
                                })
                            }
                        },
                        'feature_selection': {
                            'enabled': True,
                            'n_features': 20
                        }
                    })
                    
                    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
                    success = model.train(
                        df=data,
                        symbol=symbol_clean,
                        timeframe=timeframe
                    )
                    
                    if success:
                        symbol_models[timeframe] = model
                        logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –¥–ª—è {symbol} {timeframe} –æ–±—É—á–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
                    else:
                        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è {symbol} {timeframe}")
                        continue
                    
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è {symbol} {timeframe}: {e}")
                    continue
            
            if symbol_models:
                models[symbol] = symbol_models
        
        # === –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ===
        logger.info("=== –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ===")
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –º–æ–¥–µ–ª–µ–π
        models_dir = Path("models")
        models_dir.mkdir(exist_ok=True)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥—É—é –º–æ–¥–µ–ª—å
        for symbol, symbol_models in models.items():
            symbol_clean = symbol.replace('/', '_')
            
            for timeframe, model in symbol_models.items():
                model_path = models_dir / f"{symbol_clean}_{timeframe}_model.pkl"
                
                try:
                    model.save_model(str(model_path))
                    logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ {model_path}: {e}")
        
        # === –ó–ê–ü–£–°–ö –ë–≠–ö–¢–ï–°–¢–ê ===
        logger.info("=== –ó–ê–ü–£–°–ö –ë–≠–ö–¢–ï–°–¢–ê ===")
        
        try:
            # –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –±—ç–∫—Ç–µ—Å—Ç
            logger.info("–ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –±—ç–∫—Ç–µ—Å—Ç...")
            
            # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º –±—ç–∫—Ç–µ—Å—Ç
            import advanced_backtest
            advanced_backtest.main()
            
            logger.info("‚úÖ –ë—ç–∫—Ç–µ—Å—Ç –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –±—ç–∫—Ç–µ—Å—Ç–∞: {e}")
            logger.warning("–ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ –±—ç–∫—Ç–µ—Å—Ç–∞...")
        
        logger.info("‚úÖ Pipeline –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
        logger.info(f"–û–±—É—á–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: {sum(len(sm) for sm in models.values())}")
        logger.info("–ì–æ—Ç–æ–≤–æ –∫ —Ç–æ—Ä–≥–æ–≤–ª–µ!")
        
        return {
            'models': models,
            'processed_data': processed_data
        }
        
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ pipeline: {e}")
        raise

if __name__ == "__main__":
    main() 